[
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 64\n#define OUT_CHANNELS 128\n#define IN_HEIGHT 128\n#define IN_WIDTH 128\n#define KERNEL_SIZE 3\n#define EPSILON 1e-5\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float conv1_output[BATCH_SIZE][OUT_CHANNELS / 2][IN_HEIGHT][IN_WIDTH];\nstatic float depthwise_output[BATCH_SIZE][OUT_CHANNELS / 2][IN_HEIGHT][IN_WIDTH];\nstatic float conv2_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float branch2_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\n\nstatic unsigned long _rand_seed = 1;\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n\n\n\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\n\n// 1x1卷积 (降维)\nvoid conv1x1_reduce() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 2; oc++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    conv1_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 深度可分离卷积\nvoid depthwise_conv() {\n    int padding = 1;\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS / 2; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int kh = 0; kh < KERNEL_SIZE; kh++) {\n                        for (int kw = 0; kw < KERNEL_SIZE; kw++) {\n                            int ih = h + kh - padding;\n                            int iw = w + kw - padding;\n                            if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                sum += conv1_output[b][c][ih][iw] * 0.01;  // 假设权重为0.01\n                            }\n                        }\n                    }\n                    depthwise_output[b][c][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 1x1卷积 (升维)\nvoid conv1x1_expand() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS; oc++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < OUT_CHANNELS / 2; ic++) {\n                        sum += depthwise_output[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    conv2_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// Branch2: 1x1卷积\nvoid branch2_conv() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS; oc++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    branch2_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// Skip Connection: 逐元素相加 + ReLU\nvoid skip_connection() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = conv2_output[b][c][h][w] + branch2_output[b][c][h][w];\n                    output[b][c][h][w] = fmaxf(0.0, sum);  // ReLU\n                }\n            }\n        }\n    }\n}\n\n// 打印输出\n\n\n// 主函数\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    conv1x1_reduce();      // Branch1: 1x1 Conv (降维)\n    depthwise_conv();      // Branch1: Depthwise Conv\n    conv1x1_expand();     // Branch1: 1x1 Conv (升维)\n    branch2_conv();       // Branch2: 1x1 Conv\n    skip_connection();    // Skip Connection: Add + ReLU\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 204,\n        \"min_slack\": 0.04769998999996372,\n        \"max_frequency\": 100.47928609418965,\n        \"estimated_resources_area\": 13057,\n        \"dsp_count\": 5,\n        \"flip_flops\": 2916\n    },\n    \"silicon_compiler\": {\n        \"area\": 235087.0,\n        \"max_frequency\": 20.870073358307852,\n        \"power\": 18.089399999999998\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n//#include <math.h>\n#include <string.h>\n\n// 输入输出参数\n#define BATCH_SIZE 2\n#define IN_CHANNELS 256\n#define OUT_CHANNELS 512\n#define IN_SIZE 64\n#define NUM_HEADS 8\n#define HEAD_DIM (OUT_CHANNELS / NUM_HEADS)\n\n// 全局静态数组（预分配内存）\nfloat input_feature[BATCH_SIZE][IN_CHANNELS][IN_SIZE][IN_SIZE]; // 输入特征\nfloat output_feature[BATCH_SIZE][OUT_CHANNELS][IN_SIZE][IN_SIZE]; // 输出特征\nfloat dilated_output[BATCH_SIZE][OUT_CHANNELS][IN_SIZE][IN_SIZE]; // 空洞卷积输出\nfloat attention_output[BATCH_SIZE][OUT_CHANNELS][IN_SIZE][IN_SIZE]; // 注意力输出\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\n\nfloat exp(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n\n\n\n\n// 空洞卷积层\nvoid dilated_conv(int in_channels, int out_channels, int kernel_size, int dilation) {\n    int padding = dilation * (kernel_size - 1) / 2;\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < out_channels; oc++) {\n            for (int h = 0; h < IN_SIZE; h++) {\n                for (int w = 0; w < IN_SIZE; w++) {\n                    float sum = 0.0f;\n                    for (int ic = 0; ic < in_channels; ic++) {\n                        for (int kh = 0; kh < kernel_size; kh++) {\n                            for (int kw = 0; kw < kernel_size; kw++) {\n                                int nh = h + kh * dilation - padding;\n                                int nw = w + kw * dilation - padding;\n                                if (nh >= 0 && nh < IN_SIZE && nw >= 0 && nw < IN_SIZE) {\n                                    sum += input_feature[b][ic][nh][nw] * 0.01f; // 模拟卷积核权重\n                                }\n                            }\n                        }\n                    }\n                    dilated_output[b][oc][h][w] = fmaxf(sum, 0.0f); // ReLU\n                }\n            }\n        }\n    }\n}\n\n// 多头注意力机制\nvoid multi_head_attention(int num_heads, int head_dim) {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int h = 0; h < IN_SIZE; h++) {\n            for (int w = 0; w < IN_SIZE; w++) {\n                for (int head = 0; head < num_heads; head++) {\n                    float query = 0.0f, key = 0.0f, value = 0.0f;\n                    for (int c = 0; c < head_dim; c++) {\n                        query += dilated_output[b][head * head_dim + c][h][w] * 0.01f; // 模拟Q权重\n                        key += dilated_output[b][head * head_dim + c][h][w] * 0.01f;   // 模拟K权重\n                        value += dilated_output[b][head * head_dim + c][h][w] * 0.01f; // 模拟V权重\n                    }\n                    float attention = exp(query * key / sqrt(head_dim)); // 注意力得分\n                    attention_output[b][head * head_dim][h][w] = attention * value; // 上下文聚合\n                }\n            }\n        }\n    }\n}\n\n// 全局上下文融合\nvoid global_context_fusion() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            float channel_sum = 0.0f;\n            for (int h = 0; h < IN_SIZE; h++) {\n                for (int w = 0; w < IN_SIZE; w++) {\n                    channel_sum += attention_output[b][c][h][w];\n                }\n            }\n            float channel_attn = 1.0f / (1.0f + exp(-channel_sum / (IN_SIZE * IN_SIZE))); // 通道注意力\n            for (int h = 0; h < IN_SIZE; h++) {\n                for (int w = 0; w < IN_SIZE; w++) {\n                    output_feature[b][c][h][w] = attention_output[b][c][h][w] * channel_attn; // 通道重标定\n                }\n            }\n        }\n    }\n}\n\n// 主函数\nint main() {\n    // 模拟输入数据\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_SIZE; h++) {\n                for (int w = 0; w < IN_SIZE; w++) {\n                    input_feature[b][c][h][w] = 0.5f; // 示例输入\n                }\n            }\n        }\n    }\n\n    // 多分支空洞卷积\n    dilated_conv(IN_CHANNELS, OUT_CHANNELS, 3, 2);\n\n    // 多头注意力机制\n    multi_head_attention(NUM_HEADS, HEAD_DIM);\n\n    // 全局上下文融合\n    global_context_fusion();\n\n    // 打印输出结果\n  \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 202,\n        \"min_slack\": 5e-10,\n        \"max_frequency\": 100.000000005,\n        \"estimated_resources_area\": 11441,\n        \"dsp_count\": 0,\n        \"flip_flops\": 3975\n    },\n    \"silicon_compiler\": {\n        \"area\": 291655.0,\n        \"max_frequency\": 21.202387388819982,\n        \"power\": 23.9133\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 3\n#define OUT_CHANNELS 64\n#define IN_HEIGHT 256\n#define IN_WIDTH 256\n#define KERNEL_SIZE 3\n#define POOL_SIZE 2\n#define POOL_STRIDE 2\n#define EPSILON 1e-5\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float conv_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float bn_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float relu_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float pool_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT / POOL_STRIDE][IN_WIDTH / POOL_STRIDE];\n#define INFINITY (1.0f / 0.0f)  // 正无穷大（float 类型）\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat pow(float base, float exponent) {\n    // 验证指数必须为整数且为正数\n    if (exponent <= 0.0f) return 0.0f / 0.0f; // NaN\n    \n    // 检查是否为整数\n    unsigned int exp_int = (unsigned int)exponent;\n    if ((float)exp_int != exponent) return 0.0f / 0.0f;\n\n    // 处理特殊情况\n    if (base == 0.0f) return 0.0f;\n\n    // 负数底数处理\n    int sign = 1;\n    if (base < 0.0f) {\n        sign = (exp_int % 2 == 1) ? -1 : 1;\n        base = -base;\n    }\n\n    // 快速幂计算\n    float result = 1.0f;\n    while (exp_int > 0) {\n        if (exp_int & 1) result *= base;\n        base *= base;\n        exp_int >>= 1;\n    }\n    return sign * result;\n}\n\n\n\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = 0.0f;//(float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\nfloat sqrt(float x) {\n    if (x <= 0) return 0.0f;\n    float guess = x / 2.0f;\n    for (int i = 0; i < 10; i++) {\n        guess = 0.5f * (guess + x / guess);\n    }\n    return guess;\n}\n\n// Z-Score归一化\nvoid z_score_normalization() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            // 计算均值和标准差\n            float mean = 0.0, std = 0.0;\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    mean += input[b][c][h][w];\n                }\n            }\n            mean /= (IN_HEIGHT * IN_WIDTH);\n\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    std += pow(input[b][c][h][w] - mean, 2);\n                }\n            }\n            std = sqrt(std / (IN_HEIGHT * IN_WIDTH) + EPSILON);\n\n            // 归一化\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (input[b][c][h][w] - mean) / std;\n                }\n            }\n        }\n    }\n}\n\n// 卷积层 (3x3, stride=1, padding=1)\nvoid conv2d() {\n    int padding = 1;\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS; oc++) {\n            for (int oh = 0; oh < IN_HEIGHT; oh++) {\n                for (int ow = 0; ow < IN_WIDTH; ow++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        for (int kh = 0; kh < KERNEL_SIZE; kh++) {\n                            for (int kw = 0; kw < KERNEL_SIZE; kw++) {\n                                int ih = oh + kh - padding;\n                                int iw = ow + kw - padding;\n                                if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                    sum += input[b][ic][ih][iw] * 0.01;  // 假设权重为0.01\n                                }\n                            }\n                        }\n                    }\n                    conv_output[b][oc][oh][ow] = sum;\n                }\n            }\n        }\n    }\n}\n\n// BatchNorm层\nvoid batch_norm() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            // 计算均值和方差\n            float mean = 0.0, var = 0.0;\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    mean += conv_output[b][c][h][w];\n                }\n            }\n            mean /= (IN_HEIGHT * IN_WIDTH);\n\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    var += pow(conv_output[b][c][h][w] - mean, 2);\n                }\n            }\n            var /= (IN_HEIGHT * IN_WIDTH);\n\n            // 归一化\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    bn_output[b][c][h][w] = (conv_output[b][c][h][w] - mean) / sqrt(var + EPSILON);\n                }\n            }\n        }\n    }\n}\n\n// ReLU激活\nvoid relu() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    relu_output[b][c][h][w] = fmaxf(0.0, bn_output[b][c][h][w]);\n                }\n            }\n        }\n    }\n}\n\n// MaxPool层 (2x2, stride=2)\nvoid max_pool() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int ph = 0; ph < IN_HEIGHT / POOL_STRIDE; ph++) {\n                for (int pw = 0; pw < IN_WIDTH / POOL_STRIDE; pw++) {\n                    float max_val = -INFINITY;\n                    for (int kh = 0; kh < POOL_SIZE; kh++) {\n                        for (int kw = 0; kw < POOL_SIZE; kw++) {\n                            int h = ph * POOL_STRIDE + kh;\n                            int w = pw * POOL_STRIDE + kw;\n                            if (h < IN_HEIGHT && w < IN_WIDTH) {\n                                max_val = fmaxf(max_val, relu_output[b][c][h][w]);\n                            }\n                        }\n                    }\n                    pool_output[b][c][ph][pw] = max_val;\n                }\n            }\n        }\n    }\n}\n\n// 打印输出\n// void print_output() {\n//     //printf(\"Pooling 输出尺寸: %dx%dx%d\\n\", \n//     //       IN_HEIGHT / POOL_STRIDE, IN_WIDTH / POOL_STRIDE, OUT_CHANNELS);\n//     //printf(\"Pooling 输出示例:\\n\");\n//     for (int c = 0; c < 3; c++) {\n//       //  printf(\"通道 %d:\\n\", c);\n//         for (int h = 0; h < 3; h++) {\n//             for (int w = 0; w < 3; w++) {\n//         //        printf(\"%.4f \", pool_output[0][c][h][w]);\n//             }\n//           //  printf(\"\\n\");\n//         }\n//     }\n// }\n\n// 主函数\nint _1_cnn() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    z_score_normalization();\n    conv2d();\n    batch_norm();\n    relu();\n    max_pool();\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    z_score_normalization();\n    conv2d();\n    batch_norm();\n    relu();\n    max_pool();\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}\n",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 371,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 19978,\n        \"dsp_count\": 0,\n        \"flip_flops\": 5374\n    },\n    \"silicon_compiler\": {\n        \"area\": 418359.0,\n        \"max_frequency\": 19.28368812105528,\n        \"power\": 36.86409999999999\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <string.h>\n\n// 定义常量\n#define VOCAB_SIZE 22        // BERT 词表大小30522\n#define HIDDEN_SIZE 68       // 768隐藏层维度\n#define N_LAYERS 12           // Transformer 层数\n#define N_HEADS 12            // 多头注意力头数\n#define FFN_HIDDEN_SIZE (HIDDEN_SIZE * 4)  // 前馈网络隐藏层大小\n#define MAX_SEQ_LEN 12       //512 最大序列长度\n#define DROPOUT 0.1           // Dropout 概率\n#define DIM HIDDEN_SIZE       // 定义 dim 为 HIDDEN_SIZE\n#define HEAD_DIM (DIM / N_HEADS)  // 每个注意力头的维度\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\n\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n\n\n// 定义矩阵乘法\nvoid matmul(float* a, float* b, float* output, int m, int n, int k) {\n    for (int i = 0; i < m; i++) {\n        for (int j = 0; j < k; j++) {\n            output[i * k + j] = 0.0f;\n            for (int l = 0; l < n; l++) {\n                output[i * k + j] += a[i * n + l] * b[l * k + j];\n            }\n        }\n    }\n}\n\n// 定义 Softmax\nvoid softmax(float* x, int size) {\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) max_val = x[i];\n    }\n\n    float sum_exp = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum_exp += x[i];\n    }\n\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum_exp;\n    }\n}\n\n// 定义 Layer Normalization\nvoid layer_norm(float* x, float* gamma, float* beta, int size) {\n    float mean = 0.0f, var = 0.0f;\n    for (int i = 0; i < size; i++) {\n        mean += x[i];\n    }\n    mean /= size;\n\n    for (int i = 0; i < size; i++) {\n        var += (x[i] - mean) * (x[i] - mean);\n    }\n    var /= size;\n\n    float std = sqrtf(var + 1e-5);\n    for (int i = 0; i < size; i++) {\n        x[i] = (x[i] - mean) / std * gamma[i] + beta[i];\n    }\n}\n\n// 定义多头注意力机制\nvoid multi_head_attention(float* query, float* key, float* value, float* output) {\n    float scores[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n    float attn_weights[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n\n    // 计算注意力分数\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        for (int j = 0; j < MAX_SEQ_LEN; j++) {\n            scores[i][j] = 0.0f;\n            for (int l = 0; l < HEAD_DIM; l++) {\n                scores[i][j] += query[i * HEAD_DIM + l] * key[j * HEAD_DIM + l];\n            }\n            scores[i][j] /= sqrtf(HEAD_DIM);\n        }\n    }\n\n    // Softmax\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        softmax(scores[i], MAX_SEQ_LEN);\n    }\n\n    // 加权求和\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        for (int j = 0; j < HEAD_DIM; j++) {\n            output[i * HEAD_DIM + j] = 0.0f;\n            for (int l = 0; l < MAX_SEQ_LEN; l++) {\n                output[i * HEAD_DIM + j] += scores[i][l] * value[l * HEAD_DIM + j];\n            }\n        }\n    }\n}\n\n// 定义前馈神经网络\nvoid feed_forward(float* x, float* w1, float* w2, float* output) {\n    float h[FFN_HIDDEN_SIZE];\n    matmul(x, w1, h, 1, DIM, FFN_HIDDEN_SIZE);\n    for (int i = 0; i < FFN_HIDDEN_SIZE; i++) {\n        h[i] = fmaxf(0.0f, h[i]);  // ReLU\n    }\n    matmul(h, w2, output, 1, FFN_HIDDEN_SIZE, DIM);\n}\n\n// 定义 Transformer Block\nvoid transformer_block(float* x, float* attn_weights, float* ffn_weights) {\n    float attn_output[DIM];\n    multi_head_attention(x, x, x, attn_output);\n\n    for (int i = 0; i < DIM; i++) {\n        x[i] += attn_output[i];  // 残差连接\n    }\n\n    float ffn_output[DIM];\n    feed_forward(x, ffn_weights, ffn_weights + DIM * FFN_HIDDEN_SIZE, ffn_output);\n\n    for (int i = 0; i < DIM; i++) {\n        x[i] += ffn_output[i];  // 残差连接\n    }\n}\n\n// 定义 BERT 模型\nvoid bert(float* input, float* embedding_weights, float* transformer_weights, float* output) {\n    float x[DIM];\n    matmul(input, embedding_weights, x, 1, MAX_SEQ_LEN, DIM);\n\n    for (int i = 0; i < N_LAYERS; i++) {\n        transformer_block(x, transformer_weights, transformer_weights + DIM * DIM);\n    }\n\n    // 将最终输出复制到 output\n    memcpy(output, x, DIM * sizeof(float));\n}\n\n// 定义下一句预测（NSP）\nvoid next_sentence_prediction(float* cls_output, float* nsp_weights, float* nsp_bias, float* output) {\n    float logits[2];\n    matmul(cls_output, nsp_weights, logits, 1, HIDDEN_SIZE, 2);\n    for (int i = 0; i < 2; i++) {\n        logits[i] += nsp_bias[i];\n    }\n    softmax(logits, 2);\n    memcpy(output, logits, 2 * sizeof(float));\n}\n\n// 定义掩码语言模型（MLM）\nvoid masked_language_model(float* output, float* mlm_weights, float* mlm_bias, float* mlm_output) {\n    float logits[MAX_SEQ_LEN * VOCAB_SIZE];\n    matmul(output, mlm_weights, logits, MAX_SEQ_LEN, HIDDEN_SIZE, VOCAB_SIZE);\n    for (int i = 0; i < MAX_SEQ_LEN * VOCAB_SIZE; i++) {\n        logits[i] += mlm_bias[i % VOCAB_SIZE];\n    }\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        softmax(logits + i * VOCAB_SIZE, VOCAB_SIZE);\n    }\n    memcpy(mlm_output, logits, MAX_SEQ_LEN * VOCAB_SIZE * sizeof(float));\n}\n\nint main() {\n    // 初始化输入和权重\n    float input[MAX_SEQ_LEN] = {0};\n    float embedding_weights[VOCAB_SIZE * HIDDEN_SIZE] = {0};\n    float transformer_weights[N_LAYERS * HIDDEN_SIZE * HIDDEN_SIZE * 2] = {0};\n    float nsp_weights[HIDDEN_SIZE * 2] = {0};\n    float nsp_bias[2] = {0};\n    float mlm_weights[HIDDEN_SIZE * VOCAB_SIZE] = {0};\n    float mlm_bias[VOCAB_SIZE] = {0};\n\n    // BERT 输出\n    float bert_output[HIDDEN_SIZE];\n    bert(input, embedding_weights, transformer_weights, bert_output);\n\n    // 下一句预测\n    float nsp_output[2];\n    next_sentence_prediction(bert_output, nsp_weights, nsp_bias, nsp_output);\n\n    // 掩码语言模型\n    float mlm_output[MAX_SEQ_LEN * VOCAB_SIZE];\n    masked_language_model(bert_output, mlm_weights, mlm_bias, mlm_output);\n\n  \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 630,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 31976,\n        \"dsp_count\": 0,\n        \"flip_flops\": 18229\n    },\n    \"silicon_compiler\": {\n        \"area\": 2667930.0,\n        \"max_frequency\": 17.232495661719216,\n        \"power\": 147.769\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <string.h>\n\n// 定义常量\n#define VOCAB_SIZE 300       // 300000词表大小\n#define EMBEDDING_SIZE 128     // 分解嵌入维度\n#define HIDDEN_SIZE 128       //4096 隐藏层维度\n#define N_LAYERS 12            // Transformer 层数\n#define N_HEADS 64             // 多头注意力头数\n#define FFN_HIDDEN_SIZE 128  //16384 前馈网络隐藏层大小\n#define MAX_SEQ_LEN 64        //512 最大序列长度\n#define DROPOUT 0.1            // Dropout 概率\n#define DIM HIDDEN_SIZE        // 定义 dim 为 HIDDEN_SIZE\n#define HEAD_DIM (DIM / N_HEADS)  // 每个注意力头的维度\n\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\n\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\nvoid swap_nodes(void* a, void* b, size_t width) {\n    char* p1 = (char*)a;\n    char* p2 = (char*)b;\n    for (size_t i = 0; i < width; ++i) {\n        char temp = p1[i];\n        p1[i] = p2[i];\n        p2[i] = temp;\n    }\n}\n\n// 定义矩阵乘法\nvoid matmul(float* a, float* b, float* output, int m, int n, int k) {\n    for (int i = 0; i < m; i++) {\n        for (int j = 0; j < k; j++) {\n            output[i * k + j] = 0.0f;\n            for (int l = 0; l < n; l++) {\n                output[i * k + j] += a[i * n + l] * b[l * k + j];\n            }\n        }\n    }\n}\n\n// 定义 Softmax\nvoid softmax(float* x, int size) {\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) max_val = x[i];\n    }\n\n    float sum_exp = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum_exp += x[i];\n    }\n\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum_exp;\n    }\n}\n\n// 定义 Layer Normalization\nvoid layer_norm(float* x, float* gamma, float* beta, int size) {\n    float mean = 0.0f, var = 0.0f;\n    for (int i = 0; i < size; i++) {\n        mean += x[i];\n    }\n    mean /= size;\n\n    for (int i = 0; i < size; i++) {\n        var += (x[i] - mean) * (x[i] - mean);\n    }\n    var /= size;\n\n    float std = sqrtf(var + 1e-5);\n    for (int i = 0; i < size; i++) {\n        x[i] = (x[i] - mean) / std * gamma[i] + beta[i];\n    }\n}\n\n// 定义多头注意力机制\nvoid multi_head_attention(float* query, float* key, float* value, float* output) {\n    float scores[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n    float attn_weights[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n\n    // 计算注意力分数\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        for (int j = 0; j < MAX_SEQ_LEN; j++) {\n            scores[i][j] = 0.0f;\n            for (int l = 0; l < HEAD_DIM; l++) {\n                scores[i][j] += query[i * HEAD_DIM + l] * key[j * HEAD_DIM + l];\n            }\n            scores[i][j] /= sqrtf(HEAD_DIM);\n        }\n    }\n\n    // Softmax\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        softmax(scores[i], MAX_SEQ_LEN);\n    }\n\n    // 加权求和\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        for (int j = 0; j < HEAD_DIM; j++) {\n            output[i * HEAD_DIM + j] = 0.0f;\n            for (int l = 0; l < MAX_SEQ_LEN; l++) {\n                output[i * HEAD_DIM + j] += scores[i][l] * value[l * HEAD_DIM + j];\n            }\n        }\n    }\n}\n\n// 定义前馈神经网络\nvoid feed_forward(float* x, float* w1, float* w2, float* output) {\n    float h[FFN_HIDDEN_SIZE];\n    matmul(x, w1, h, 1, DIM, FFN_HIDDEN_SIZE);\n    for (int i = 0; i < FFN_HIDDEN_SIZE; i++) {\n        h[i] = fmaxf(0.0f, h[i]);  // ReLU\n    }\n    matmul(h, w2, output, 1, FFN_HIDDEN_SIZE, DIM);\n}\n\n// 定义 Transformer Block\nvoid transformer_block(float* x, float* attn_weights, float* ffn_weights) {\n    float attn_output[DIM];\n    multi_head_attention(x, x, x, attn_output);\n\n    for (int i = 0; i < DIM; i++) {\n        x[i] += attn_output[i];  // 残差连接\n    }\n\n    float ffn_output[DIM];\n    feed_forward(x, ffn_weights, ffn_weights + DIM * FFN_HIDDEN_SIZE, ffn_output);\n\n    for (int i = 0; i < DIM; i++) {\n        x[i] += ffn_output[i];  // 残差连接\n    }\n}\n\n// 定义 ALBERT 模型\nvoid albert(float* input, float* embedding_weights, float* transformer_weights, float* output) {\n    float x[DIM]={0};\n    matmul(input, embedding_weights, x, 1, MAX_SEQ_LEN, DIM);\n\n    // 参数共享：所有层共享同一组权重\n    for (int i = 0; i < N_LAYERS; i++) {\n        transformer_block(x, transformer_weights, transformer_weights + DIM * DIM);\n    }\n\n    // 将最终输出复制到 output\n    memcpy(output, x, DIM * sizeof(float));\n}\n\n// 定义下一句预测（SOP）\nvoid sentence_order_prediction(float* cls_output, float* sop_weights, float* sop_bias, float* output) {\n    float logits[2];\n    matmul(cls_output, sop_weights, logits, 1, HIDDEN_SIZE, 2);\n    for (int i = 0; i < 2; i++) {\n        logits[i] += sop_bias[i];\n    }\n    softmax(logits, 2);\n    memcpy(output, logits, 2 * sizeof(float));\n}\n\n// 定义掩码语言模型（MLM）\nvoid masked_language_model(float* output, float* mlm_weights, float* mlm_bias, float* mlm_output) {\n    float logits[MAX_SEQ_LEN * VOCAB_SIZE];\n    matmul(output, mlm_weights, logits, MAX_SEQ_LEN, HIDDEN_SIZE, VOCAB_SIZE);\n    for (int i = 0; i < MAX_SEQ_LEN * VOCAB_SIZE; i++) {\n        logits[i] += mlm_bias[i % VOCAB_SIZE];\n    }\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        softmax(logits + i * VOCAB_SIZE, VOCAB_SIZE);\n    }\n    memcpy(mlm_output, logits, MAX_SEQ_LEN * VOCAB_SIZE * sizeof(float));\n}\n\nint main() {\n    // 初始化输入和权重\n    float input[MAX_SEQ_LEN] = {0};\n    float embedding_weights[VOCAB_SIZE * EMBEDDING_SIZE] = {0};\n    float transformer_weights[N_LAYERS * HIDDEN_SIZE * HIDDEN_SIZE * 2] = {0};\n    float sop_weights[HIDDEN_SIZE * 2] = {0};\n    float sop_bias[2] = {0};\n    float mlm_weights[HIDDEN_SIZE * VOCAB_SIZE] = {0};\n    float mlm_bias[VOCAB_SIZE] = {0};\n\n    // ALBERT 输出\n    float albert_output[HIDDEN_SIZE];\n    albert(input, embedding_weights, transformer_weights, albert_output);\n\n    \n    float sop_output[2];\n    sentence_order_prediction(albert_output, sop_weights, sop_bias, sop_output);\n\n    // 掩码语言模型\n    float mlm_output[MAX_SEQ_LEN * VOCAB_SIZE];\n    masked_language_model(albert_output, mlm_weights, mlm_bias, mlm_output);\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 295,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 28404,\n        \"dsp_count\": 0,\n        \"flip_flops\": 6423\n    },\n    \"silicon_compiler\": {\n        \"area\": 585140.0,\n        \"max_frequency\": 17.391122875239564,\n        \"power\": 55.2044\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n#include <string.h>\n#include <float.h>\n\n#define DIM 128 //128\n#define NUM_HEADS 8\n#define FF_DIM 128 //2048\n#define MAX_SEQ_LEN 64//512\n#define VOCAB_SIZE 128 //32128\n#define BEAM_SIZE 4\n#define NUM_LAYERS 12\n#define PREFIX_LEN 10\n#define EOS_TOKEN 1\n\ntypedef float Vector[DIM];\n\ntypedef struct {\n    float ffn_w1[NUM_LAYERS][DIM][FF_DIM];\n    float ffn_w2[NUM_LAYERS][FF_DIM][DIM];\n    Vector ln_gamma[NUM_LAYERS][2];\n    Vector ln_beta[NUM_LAYERS][2];\n    float vocab_weights[DIM][VOCAB_SIZE];\n    float prefix_weights[PREFIX_LEN][DIM];\n} T5Weights;\n\ntypedef struct {\n    int tokens[MAX_SEQ_LEN];\n    float score;\n    int length;\n} BeamNode;\n\nstatic unsigned long _rand_seed = 1;\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\nvoid swap_nodes(void* a, void* b, size_t width) {\n    char* p1 = (char*)a;\n    char* p2 = (char*)b;\n    for (size_t i = 0; i < width; ++i) {\n        char temp = p1[i];\n        p1[i] = p2[i];\n        p2[i] = temp;\n    }\n}\n\n// 手动实现的冒泡排序版qsort\nvoid qsort(void* base, size_t num, size_t width, \n                int (*cmp)(const void*, const void*)) \n{\n    char* array = (char*)base;\n    for (size_t i = 0; i < num - 1; ++i) {\n        for (size_t j = 0; j < num - 1 - i; ++j) {\n            // 计算相邻元素地址[2,4](@ref)\n            void* elem1 = array + j * width;\n            void* elem2 = array + (j + 1) * width;\n            \n            // 调用比较函数[1,3](@ref)\n            if (cmp(elem1, elem2) > 0) { \n                swap_nodes(elem1, elem2, width); // 执行交换\n            }\n        }\n    }\n}\n\n\n\n// 快速幂算法（仅支持正整数指数）\nfloat powf(float base, float exponent) {\n    // 验证指数必须为整数且为正数\n    if (exponent <= 0.0f) return 0.0f / 0.0f; // NaN\n    \n    // 检查是否为整数\n    unsigned int exp_int = (unsigned int)exponent;\n    if ((float)exp_int != exponent) return 0.0f / 0.0f;\n\n    // 处理特殊情况\n    if (base == 0.0f) return 0.0f;\n\n    // 负数底数处理\n    int sign = 1;\n    if (base < 0.0f) {\n        sign = (exp_int % 2 == 1) ? -1 : 1;\n        base = -base;\n    }\n\n    // 快速幂计算\n    float result = 1.0f;\n    while (exp_int > 0) {\n        if (exp_int & 1) result *= base;\n        base *= base;\n        exp_int >>= 1;\n    }\n    return sign * result;\n}\n\n// 指数函数近似（泰勒展开）\n\n\n// 平方根（牛顿迭代法）\nfloat sqrtf(float x) {\n    if (x <= 0) return 0.0f;\n    float guess = x / 2.0f;\n    for (int i = 0; i < 10; i++) {\n        guess = 0.5f * (guess + x / guess);\n    }\n    return guess;\n}\n\n// 添加矩阵乘法和softmax实现\nvoid matmul(float* A, float* B, float* C, int m, int k, int n) {\n    memset(C, 0, m*n*sizeof(float));\n    for (int i = 0; i < m; i++) {\n        for (int j = 0; j < n; j++) {\n            for (int l = 0; l < k; l++) {\n                C[i*n + j] += A[i*k + l] * B[l*n + j];\n            }\n        }\n    }\n}\n\nvoid softmax(float* arr, int size) {\n    float max_val = -FLT_MAX;\n    for (int i = 0; i < size; ++i)\n        if (arr[i] > max_val) max_val = arr[i];\n    \n    float sum = 0.0f;\n    for (int i = 0; i < size; ++i) {\n        arr[i] = expf(arr[i] - max_val);\n        sum += arr[i];\n    }\n    for (int i = 0; i < size; ++i)\n        arr[i] /= sum;\n}\n\nvoid t5_relative_bias(int seq_len, float bias[MAX_SEQ_LEN][MAX_SEQ_LEN]) {\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < seq_len; j++) {\n            int distance = i - j + (seq_len - 1);\n            bias[i][j] = (distance < 0) ? -FLT_MAX : 1.0f / sqrtf(distance + 1);\n        }\n    }\n}\n\nvoid layer_norm(float* x, Vector gamma, Vector beta) {\n    float mean = 0.0f, var = 0.0f;\n    for (int i = 0; i < DIM; i++) mean += x[i];\n    mean /= DIM;\n    for (int i = 0; i < DIM; i++) var += (x[i] - mean) * (x[i] - mean);\n    var /= DIM;\n    for (int i = 0; i < DIM; i++)\n        x[i] = (x[i] - mean)/sqrtf(var + 1e-6) * gamma[i] + beta[i];\n}\n\nvoid attention(float* x, float rel_bias[MAX_SEQ_LEN][MAX_SEQ_LEN], \n               float* output, int seq_len) {\n    float scores[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n    \n    // 计算注意力分数\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < seq_len; j++) {\n            for (int k = 0; k < DIM; k++) {\n                scores[i][j] += x[i*DIM + k] * x[j*DIM + k];\n            }\n            scores[i][j] = scores[i][j]/sqrtf(DIM) + rel_bias[i][j];\n        }\n        softmax(scores[i], seq_len);\n    }\n\n    // 计算加权和\n    for (int i = 0; i < seq_len; i++) {\n        for (int k = 0; k < DIM; k++) {\n            output[i*DIM + k] = 0;\n            for (int j = 0; j < seq_len; j++) {\n                output[i*DIM + k] += scores[i][j] * x[j*DIM + k];\n            }\n        }\n    }\n}\n\nvoid cross_attention(float* q, float* kv, float* output, \n                    int q_len, int kv_len) {\n    // 简化的cross-attention实现\n    for (int i = 0; i < q_len; i++) {\n        for (int j = 0; j < kv_len; j++) {\n            float score = 0;\n            for (int k = 0; k < DIM; k++) {\n                score += q[i*DIM + k] * kv[j*DIM + k];\n            }\n            score /= sqrtf(DIM);\n            \n            // 计算加权和\n            if (j == 0) {\n                for (int k = 0; k < DIM; k++) \n                    output[i*DIM + k] = score * kv[j*DIM + k];\n            } else {\n                for (int k = 0; k < DIM; k++) \n                    output[i*DIM + k] += score * kv[j*DIM + k];\n            }\n        }\n    }\n}\n\nvoid transformer_encoder_layer(float* x, int seq_len, int layer_idx,\n                              T5Weights* w) {\n    float rel_bias[MAX_SEQ_LEN][MAX_SEQ_LEN];\n    t5_relative_bias(seq_len, rel_bias);\n\n    // 自注意力\n    float attn_out[MAX_SEQ_LEN * DIM];\n    attention(x, rel_bias, attn_out, seq_len);\n    \n    // 残差连接+LayerNorm\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += attn_out[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][0], w->ln_beta[layer_idx][0]);\n    }\n\n    // 前馈网络\n    float ffn_out[MAX_SEQ_LEN * FF_DIM];\n    matmul(x, (float*)w->ffn_w1[layer_idx], ffn_out, seq_len, DIM, FF_DIM);\n    \n    // ReLU激活\n    for (int i = 0; i < seq_len*FF_DIM; i++)\n        ffn_out[i] = fmaxf(ffn_out[i], 0.0f);\n    \n    float ffn_final[MAX_SEQ_LEN * DIM];\n    matmul(ffn_out, (float*)w->ffn_w2[layer_idx], ffn_final, seq_len, FF_DIM, DIM);\n    \n    // 残差连接+LayerNorm\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += ffn_final[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][1], w->ln_beta[layer_idx][1]);\n    }\n}\n\nvoid transformer_decoder_layer(float* x, float* enc_out, int seq_len,\n                              int layer_idx, T5Weights* w) {\n    // 因果掩码\n    float causal_mask[MAX_SEQ_LEN][MAX_SEQ_LEN];\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < seq_len; j++) {\n            causal_mask[i][j] = (j > i) ? -FLT_MAX : 0.0f;\n        }\n    }\n\n    // 自注意力\n    float self_attn_out[MAX_SEQ_LEN * DIM];\n    attention(x, causal_mask, self_attn_out, seq_len);\n    \n    // 残差+LN\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += self_attn_out[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][0], w->ln_beta[layer_idx][0]);\n    }\n\n    // Cross-attention\n    float cross_attn_out[MAX_SEQ_LEN * DIM];\n    cross_attention(x, enc_out, cross_attn_out, seq_len, MAX_SEQ_LEN);\n    \n    // 残差+LN\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += cross_attn_out[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][1], w->ln_beta[layer_idx][1]);\n    }\n\n    // 前馈网络\n    float ffn_out[MAX_SEQ_LEN * FF_DIM];\n    matmul(x, (float*)w->ffn_w1[layer_idx], ffn_out, seq_len, DIM, FF_DIM);\n    \n    for (int i = 0; i < seq_len*FF_DIM; i++)\n        ffn_out[i] = fmaxf(ffn_out[i], 0.0f);\n    \n    float ffn_final[MAX_SEQ_LEN * DIM];\n    matmul(ffn_out, (float*)w->ffn_w2[layer_idx], ffn_final, seq_len, FF_DIM, DIM);\n    \n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += ffn_final[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][1], w->ln_beta[layer_idx][1]);\n    }\n}\n\nvoid encoder(float* x, int seq_len, T5Weights* w) {\n    for (int layer = 0; layer < NUM_LAYERS; layer++)\n        transformer_encoder_layer(x, seq_len, layer, w);\n}\n\nvoid decoder(float* x, float* enc_out, int seq_len, T5Weights* w) {\n    for (int layer = 0; layer < NUM_LAYERS; layer++)\n        transformer_decoder_layer(x, enc_out, seq_len, layer, w);\n}\n\nvoid apply_prefix(float* x, int seq_len, T5Weights* w) {\n    for (int i = 0; i < PREFIX_LEN; i++)\n        memcpy(x + (seq_len + i)*DIM, w->prefix_weights[i], DIM*sizeof(float));\n}\n\nint compare_nodes(const void* a, const void* b) {\n    return ((BeamNode*)b)->score > ((BeamNode*)a)->score ? 1 : -1;\n}\n\nvoid beam_search(T5Weights* w, float* enc_out) {\n    BeamNode beams[BEAM_SIZE] = {0};\n    beams[0].score = 0.0f;\n    beams[0].tokens[0] = 0;  // Start token\n    beams[0].length = 1;\n    int current_beam_size = 1;\n\n    for (int step = 0; step < MAX_SEQ_LEN; step++) {\n        BeamNode new_beams[BEAM_SIZE * VOCAB_SIZE] = {0};\n        int new_count = 0;\n\n        for (int i = 0; i < current_beam_size; i++) {\n            float input[MAX_SEQ_LEN * DIM] = {0};\n            memcpy(input, enc_out, MAX_SEQ_LEN*DIM*sizeof(float));\n\n            // 生成decoder输入\n            int curr_len = beams[i].length;\n            for (int j = 0; j < curr_len; j++)\n                input[j*DIM] = (float)beams[i].tokens[j];\n\n            decoder(input, enc_out, curr_len, w);\n\n            // 获取logits\n            float logits[VOCAB_SIZE] = {0};\n            matmul(input + (curr_len-1)*DIM, (float*)w->vocab_weights, \n                  logits, 1, DIM, VOCAB_SIZE);\n            \n            softmax(logits, VOCAB_SIZE);\n\n            // 收集候选\n            for (int j = 0; j < VOCAB_SIZE; j++) {\n                if (new_count >= BEAM_SIZE * VOCAB_SIZE) break;\n                \n                new_beams[new_count] = beams[i];\n                new_beams[new_count].tokens[curr_len] = j;\n                new_beams[new_count].score += logf(logits[j] + 1e-12);\n                new_beams[new_count].length = curr_len + 1;\n                new_count++;\n            }\n        }\n\n        qsort(new_beams, new_count, sizeof(BeamNode), compare_nodes);\n\n        current_beam_size = 0;\n        for (int i = 0; i < BEAM_SIZE && i < new_count; i++) {\n            \n            beams[current_beam_size++] = new_beams[i];\n        }\n    }\n}\n\nvoid init_weights(T5Weights* w) {\n    for (int l = 0; l < NUM_LAYERS; l++) {\n        for (int i = 0; i < DIM; i++) {\n            for (int j = 0; j < FF_DIM; j++) {\n                w->ffn_w1[l][i][j] = (rand()/(float)RAND_MAX - 0.5f) * 0.1f;\n                w->ffn_w2[l][j][i] = (rand()/(float)RAND_MAX - 0.5f) * 0.1f;\n            }\n            w->ln_gamma[l][0][i] = 1.0f;\n            w->ln_beta[l][0][i] = 0.0f;\n            w->ln_gamma[l][1][i] = 1.0f;\n            w->ln_beta[l][1][i] = 0.0f;\n        }\n    }\n    for (int i = 0; i < DIM; i++) {\n        for (int j = 0; j < VOCAB_SIZE; j++) {\n            w->vocab_weights[i][j] = (rand()/(float)RAND_MAX - 0.5f) * 0.1f;\n        }\n    }\n    for (int i = 0; i < PREFIX_LEN; i++) {\n        for (int j = 0; j < DIM; j++) {\n            w->prefix_weights[i][j] = (rand()/(float)RAND_MAX - 0.5f) * 0.1f;\n        }\n    }\n}\n\nint main() {\n    T5Weights weights;\n    init_weights(&weights);\n\n    float enc_input[MAX_SEQ_LEN * DIM] = {0};\n    int seq_len = 5;\n    apply_prefix(enc_input, seq_len, &weights);\n    \n    encoder(enc_input, seq_len + PREFIX_LEN, &weights);\n    \n    beam_search(&weights, enc_input);\n    \n    \n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 1381,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 42291,\n        \"dsp_count\": 12,\n        \"flip_flops\": 19635\n    },\n    \"silicon_compiler\": {\n        \"area\": 1573790.0,\n        \"max_frequency\": 20.976145926851984,\n        \"power\": 511.047\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n// #include <math.h>\n#include <string.h>\ndouble expf(double x)\n{\n    double result = 1.0; // 初始化\n    double term = 1.0;   // 泰勒展开的第一个项\n    for (int i = 1; i <= 10; ++i)\n    {\n        term *= x / i;  // 计算下一项\n        result += term; // 累加到结果中\n    }\n    return result;\n}\nfloat sqrtf(float x) {\n    if (x <= 0) return 0.0f;  // 防止负数\n    float guess = x / 2.0f;  // 初始猜测值\n    float result;\n    \n    // 牛顿迭代法\n    for (int i = 0; i < 10; i++) {\n        result = 0.5f * (guess + x / guess);\n        guess = result;\n    }\n\n    return result;\n}\n// 定义常量\n#define DIM  64           // 模型维度\n#define N_HEADS 8         // 多头注意力头数\n#define HEAD_DIM (DIM / N_HEADS) // 每个头的维度\n#define MAX_SEQ_LEN 64   // 最大序列长度\n#define EPS 1e-5          // 归一化的 epsilon\n#define N_LAYERS 6        // Transformer 层数\n#define HIDDEN_DIM (DIM * 4) // 前馈网络的隐藏层维度\n\n// 定义数组大小\n#define TOKENS_SIZE MAX_SEQ_LEN\n#define TOK_EMBEDDINGS_SIZE (DIM * MAX_SEQ_LEN)\n#define OUTPUT_WEIGHTS_SIZE (DIM * MAX_SEQ_LEN)\n#define ATTN_NORM_WEIGHTS_SIZE DIM\n#define FFN_NORM_WEIGHTS_SIZE DIM\n#define WQ_SIZE (DIM * DIM)\n#define WK_SIZE (DIM * DIM)\n#define WV_SIZE (DIM * DIM)\n#define WO_SIZE (DIM * DIM)\n#define W1_SIZE (DIM * HIDDEN_DIM)\n#define W2_SIZE (HIDDEN_DIM * DIM)\n#define W3_SIZE (DIM * HIDDEN_DIM)\n\n// RMSNorm 层\nvoid rms_norm(float* x, float* weight, float* output) {\n    float mean_square = 0.0f;\n    for (int i = 0; i < DIM; i++) {\n        mean_square += x[i] * x[i];\n    }\n    mean_square /= DIM;\n\n    float rsqrt_val = 1.0f / sqrtf(mean_square + EPS);\n    for (int i = 0; i < DIM; i++) {\n        output[i] = x[i] * rsqrt_val * weight[i];\n    }\n}\n\n// 矩阵乘法\nvoid matmul(float* a, float* b, float* output, int m, int n, int k) {\n    for (int i = 0; i < m; i++) {\n        for (int j = 0; j < k; j++) {\n            output[i * k + j] = 0.0f;\n            for (int l = 0; l < n; l++) {\n                output[i * k + j] += a[i * n + l] * b[l * k + j];\n            }\n        }\n    }\n}\n\n// 注意力机制\nvoid attention(float* q, float* k, float* v, float* output, int seq_len) {\n    float scores[MAX_SEQ_LEN * MAX_SEQ_LEN];\n    float attn_weights[MAX_SEQ_LEN * MAX_SEQ_LEN];\n\n    // 计算注意力分数\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < seq_len; j++) {\n            scores[i * seq_len + j] = 0.0f;\n            for (int l = 0; l < HEAD_DIM; l++) {\n                scores[i * seq_len + j] += q[i * HEAD_DIM + l] * k[j * HEAD_DIM + l];\n            }\n            scores[i * seq_len + j] /= sqrtf(HEAD_DIM);\n        }\n    }\n\n    // Softmax\n    for (int i = 0; i < seq_len; i++) {\n        float max_score = scores[i * seq_len];\n        for (int j = 1; j < seq_len; j++) {\n            if (scores[i * seq_len + j] > max_score) {\n                max_score = scores[i * seq_len + j];\n            }\n        }\n\n        float sum_exp = 0.0f;\n        for (int j = 0; j < seq_len; j++) {\n            attn_weights[i * seq_len + j] = expf(scores[i * seq_len + j] - max_score);\n            sum_exp += attn_weights[i * seq_len + j];\n        }\n\n        for (int j = 0; j < seq_len; j++) {\n            attn_weights[i * seq_len + j] /= sum_exp;\n        }\n    }\n\n    // 加权求和\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < HEAD_DIM; j++) {\n            output[i * HEAD_DIM + j] = 0.0f;\n            for (int l = 0; l < seq_len; l++) {\n                output[i * HEAD_DIM + j] += attn_weights[i * seq_len + l] * v[l * HEAD_DIM + j];\n            }\n        }\n    }\n}\n\n// 前馈神经网络\nvoid feed_forward(float* x, float* w1, float* w2, float* w3, float* output) {\n    float h1[HIDDEN_DIM];\n    float h2[HIDDEN_DIM];\n\n    // w1(x) 和 SiLU 激活\n    matmul(x, w1, h1, 1, DIM, HIDDEN_DIM);\n    for (int i = 0; i < HIDDEN_DIM; i++) {\n        h1[i] = h1[i] / (1 + expf(-h1[i])); // SiLU\n    }\n\n    // w3(x)\n    matmul(x, w3, h2, 1, DIM, HIDDEN_DIM);\n\n    // w2(SiLU(w1(x)) * w3(x))\n    for (int i = 0; i < HIDDEN_DIM; i++) {\n        h2[i] *= h1[i];\n    }\n    matmul(h2, w2, output, 1, HIDDEN_DIM, DIM);\n}\n\n// Transformer 层\nvoid transformer_block(float* x, float* attn_norm_weight, float* ffn_norm_weight,\n                       float* wq, float* wk, float* wv, float* wo,\n                       float* w1, float* w2, float* w3, float* output, int seq_len) {\n    float norm_output[DIM];\n    float attn_output[DIM];\n    float ffn_output[DIM];\n\n    // 注意力层\n    rms_norm(x, attn_norm_weight, norm_output);\n    attention(norm_output, norm_output, norm_output, attn_output, seq_len);\n    for (int i = 0; i < DIM; i++) {\n        attn_output[i] += x[i];\n    }\n\n    // 前馈层\n    rms_norm(attn_output, ffn_norm_weight, norm_output);\n    feed_forward(norm_output, w1, w2, w3, ffn_output);\n    for (int i = 0; i < DIM; i++) {\n        output[i] = attn_output[i] + ffn_output[i];\n    }\n}\n\n// Transformer 模型\nvoid transformer(float* tokens, float* tok_embeddings, float* output_weights,\n                 float* attn_norm_weights, float* ffn_norm_weights,\n                 float* wq, float* wk, float* wv, float* wo,\n                 float* w1, float* w2, float* w3, int seq_len) {\n    float h[DIM];\n    matmul(tokens, tok_embeddings, h, 1, seq_len, DIM);\n\n    for (int i = 0; i < N_LAYERS; i++) {\n        transformer_block(h, attn_norm_weights, ffn_norm_weights,\n                         wq, wk, wv, wo, w1, w2, w3, h, seq_len);\n    }\n\n    rms_norm(h, attn_norm_weights, h);\n    matmul(h, output_weights, tokens, 1, DIM, seq_len);\n}\n\nint main() {\n    int seq_len = 10;\n\n    // 定义静态数组\n    float tokens[TOKENS_SIZE];\n    float tok_embeddings[TOK_EMBEDDINGS_SIZE];\n    float output_weights[OUTPUT_WEIGHTS_SIZE];\n    float attn_norm_weights[ATTN_NORM_WEIGHTS_SIZE];\n    float ffn_norm_weights[FFN_NORM_WEIGHTS_SIZE];\n    float wq[WQ_SIZE];\n    float wk[WK_SIZE];\n    float wv[WV_SIZE];\n    float wo[WO_SIZE];\n    float w1[W1_SIZE];\n    float w2[W2_SIZE];\n    float w3[W3_SIZE];\n\n    // 初始化数据\n    for (int i = 0; i < seq_len; i++) {\n        tokens[i] = 1.0f; // 假设输入全为 1\n    }\n    for (int i = 0; i < DIM * seq_len; i++) {\n        tok_embeddings[i] = 0.01f * (i % DIM); // 假设嵌入权重\n        output_weights[i] = 0.01f * (i % DIM); // 假设输出权重\n    }\n    for (int i = 0; i < DIM; i++) {\n        attn_norm_weights[i] = 1.0f; // 假设归一化权重\n        ffn_norm_weights[i] = 1.0f;\n    }\n    for (int i = 0; i < DIM * DIM; i++) {\n        wq[i] = 0.01f * (i % DIM); // 假设注意力权重\n        wk[i] = 0.01f * (i % DIM);\n        wv[i] = 0.01f * (i % DIM);\n        wo[i] = 0.01f * (i % DIM);\n    }\n    for (int i = 0; i < DIM * HIDDEN_DIM; i++) {\n        w1[i] = 0.01f * (i % DIM); // 假设前馈权重\n        w3[i] = 0.01f * (i % DIM);\n    }\n    for (int i = 0; i < HIDDEN_DIM * DIM; i++) {\n        w2[i] = 0.01f * (i % DIM);\n    }\n\n    // 调用 Transformer\n    transformer(tokens, tok_embeddings, output_weights,\n               attn_norm_weights, ffn_norm_weights,\n               wq, wk, wv, wo, w1, w2, w3, seq_len);\n\n    // 打印输出\n  \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 917,\n        \"min_slack\": 3.4927616354707425e-13,\n        \"max_frequency\": 100.0000000000035,\n        \"estimated_resources_area\": 30438,\n        \"dsp_count\": 0,\n        \"flip_flops\": 12966\n    },\n    \"silicon_compiler\": {\n        \"area\": 1326080.0,\n        \"max_frequency\": 18.957705359343304,\n        \"power\": 145.74300000000002\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <string.h>\n\n// ================== 自定义数学函数实现 ==================\n// 伪随机数生成器（线性同余算法）\nstatic unsigned long _rand_seed = 1;\n\nint my_rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n\n// 双曲正切近似计算\nfloat my_tanhf(float x) {\n    // 限制输入范围防止溢出\n    if (x > 4.97f) return 1.0f;\n    if (x < -4.97f) return -1.0f;\n    \n    float x2 = x * x;\n    float x3 = x * x2;\n    float x5 = x3 * x2;\n    float x7 = x5 * x2;\n    return x - x3/3.0f + 2.0f*x5/15.0f - 17.0f*x7/315.0f;\n}\n\n// 快速幂算法（仅支持正整数指数）\nfloat powf(float base, float exponent) {\n    // 验证指数必须为整数且为正数\n    if (exponent <= 0.0f) return 0.0f / 0.0f; // NaN\n    \n    // 检查是否为整数\n    unsigned int exp_int = (unsigned int)exponent;\n    if ((float)exp_int != exponent) return 0.0f / 0.0f;\n\n    // 处理特殊情况\n    if (base == 0.0f) return 0.0f;\n\n    // 负数底数处理\n    int sign = 1;\n    if (base < 0.0f) {\n        sign = (exp_int % 2 == 1) ? -1 : 1;\n        base = -base;\n    }\n\n    // 快速幂计算\n    float result = 1.0f;\n    while (exp_int > 0) {\n        if (exp_int & 1) result *= base;\n        base *= base;\n        exp_int >>= 1;\n    }\n    return sign * result;\n}\n\n// 指数函数近似（泰勒展开）\ndouble expf(double x) {\n    double result = 1.0;\n    double term = 1.0;\n    for (int i = 1; i <= 10; ++i) {\n        term *= x / i;\n        result += term;\n    }\n    return result;\n}\n\n\n// 平方根（牛顿迭代法）\nfloat sqrtf(float x) {\n    if (x <= 0) return 0.0f;\n    float guess = x / 2.0f;\n    for (int i = 0; i < 10; i++) {\n        guess = 0.5f * (guess + x / guess);\n    }\n    return guess;\n}\n\n// ================== 模型配置参数 ==================\n#define HIDDEN_SIZE 64\n#define NUM_LAYERS 10\n#define NUM_HEADS 8\n#define SEQ_LEN 64\n#define VOCAB_SIZE 128\n#define INTERMEDIATE_SIZE 256\n#define BATCH_SIZE 2\n#define EPS 1e-5\n\n// ================== 模型数据结构 ==================\nstatic float embeddings[VOCAB_SIZE][HIDDEN_SIZE];\nstatic float position_emb[SEQ_LEN][HIDDEN_SIZE];\nstatic float layer_norm_buffer[BATCH_SIZE][HIDDEN_SIZE];\nstatic float attention_output[BATCH_SIZE][SEQ_LEN][HIDDEN_SIZE];\nstatic float ff_intermediate[BATCH_SIZE][INTERMEDIATE_SIZE];\nstatic float layer_output[NUM_LAYERS][BATCH_SIZE][HIDDEN_SIZE];\n\n// ================== 模型核心实现 ==================\nvoid init_parameters() {\n    for (int i = 0; i < VOCAB_SIZE; i++) {\n        for (int j = 0; j < HIDDEN_SIZE; j++) {\n            embeddings[i][j] = (float)my_rand()/32767.0f * 0.02 - 0.01;\n        }\n    }\n    \n    for (int i = 0; i < SEQ_LEN; i++) {\n        for (int j = 0; j < HIDDEN_SIZE; j++) {\n            position_emb[i][j] = (float)my_rand()/32767.0f * 0.02 - 0.01;\n        }\n    }\n}\n\nvoid absolute_position_embedding(int batch_size) {\n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < SEQ_LEN; i++) {\n            for (int j = 0; j < HIDDEN_SIZE; j++) {\n                layer_output[0][b][i*HIDDEN_SIZE+j] += position_emb[i][j];\n            }\n        }\n    }\n}\n\nvoid layer_norm(int batch_size, float input[BATCH_SIZE][HIDDEN_SIZE]) {\n    for (int b = 0; b < batch_size; b++) {\n        float mean = 0.0, var = 0.0;\n        for (int i = 0; i < HIDDEN_SIZE; i++) mean += input[b][i];\n        mean /= HIDDEN_SIZE;\n        \n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            float diff = input[b][i] - mean;\n            var += diff * diff;\n        }\n        var /= HIDDEN_SIZE;\n        \n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            layer_norm_buffer[b][i] = (input[b][i] - mean) / sqrtf(var + EPS);\n        }\n    }\n}\n\nvoid multi_head_attention(int layer_id, int batch_size) {\n    const int head_dim = HIDDEN_SIZE / NUM_HEADS;\n    static float q_proj[NUM_LAYERS][HIDDEN_SIZE][HIDDEN_SIZE];\n    static float k_proj[NUM_LAYERS][HIDDEN_SIZE][HIDDEN_SIZE];\n    static float v_proj[NUM_LAYERS][HIDDEN_SIZE][HIDDEN_SIZE];\n    \n    // 初始化投影矩阵\n    for (int i = 0; i < HIDDEN_SIZE; i++) {\n        for (int j = 0; j < HIDDEN_SIZE; j++) {\n            q_proj[layer_id][i][j] = 0.01f * (i + j);\n            k_proj[layer_id][i][j] = 0.01f * (i - j);\n            v_proj[layer_id][i][j] = 0.01f * (i * j);\n        }\n    }\n    \n    // 计算注意力\n    for (int b = 0; b < batch_size; b++) {\n        for (int h = 0; h < NUM_HEADS; h++) {\n            for (int i = 0; i < SEQ_LEN; i++) {\n                for (int j = 0; j < SEQ_LEN; j++) {\n                    float score = 0.0;\n                    for (int k = 0; k < head_dim; k++) {\n                        float q = 0.0, k_val = 0.0;\n                        for (int m = 0; m < HIDDEN_SIZE; m++) {\n                            q += layer_output[layer_id][b][m] * q_proj[layer_id][m][h*head_dim + k];\n                            k_val += layer_output[layer_id][b][m] * k_proj[layer_id][m][h*head_dim + k];\n                        }\n                        score += q * k_val;\n                    }\n                    score /= sqrtf(head_dim);\n                    float attention = expf(score) / (1.0f + expf(score));\n                    \n                    for (int k = 0; k < head_dim; k++) {\n                        float v = 0.0;\n                        for (int m = 0; m < HIDDEN_SIZE; m++) {\n                            v += layer_output[layer_id][b][m] * v_proj[layer_id][m][h*head_dim + k];\n                        }\n                        attention_output[b][i][h*head_dim + k] += attention * v;\n                    }\n                }\n            }\n        }\n    }\n}\n\nvoid feed_forward(int layer_id, int batch_size) {\n    static float w1[NUM_LAYERS][HIDDEN_SIZE][INTERMEDIATE_SIZE];\n    static float w2[NUM_LAYERS][INTERMEDIATE_SIZE][HIDDEN_SIZE];\n    \n    // 初始化权重\n    for (int i = 0; i < HIDDEN_SIZE; i++) {\n        for (int j = 0; j < INTERMEDIATE_SIZE; j++) {\n            w1[layer_id][i][j] = 0.01f * (i + j);\n        }\n    }\n    \n    for (int i = 0; i < INTERMEDIATE_SIZE; i++) {\n        for (int j = 0; j < HIDDEN_SIZE; j++) {\n            w2[layer_id][i][j] = 0.01f * (i - j);\n        }\n    }\n    \n    // 中间层计算\n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < INTERMEDIATE_SIZE; i++) {\n            ff_intermediate[b][i] = 0;\n            for (int j = 0; j < HIDDEN_SIZE; j++) {\n                ff_intermediate[b][i] += layer_norm_buffer[b][j] * w1[layer_id][j][i];\n            }\n            // GELU激活函数\n            ff_intermediate[b][i] = 0.5f * ff_intermediate[b][i] * \n                                   (1.0f + my_tanhf(0.7978845608f * \n                                   (ff_intermediate[b][i] + 0.044715f * powf(ff_intermediate[b][i], 3))));\n        }\n    }\n    \n    // 输出层\n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            layer_output[layer_id+1][b][i] = 0;\n            for (int j = 0; j < INTERMEDIATE_SIZE; j++) {\n                layer_output[layer_id+1][b][i] += ff_intermediate[b][j] * w2[layer_id][j][i];\n            }\n        }\n    }\n}\n\nvoid transformer_layer(int layer_id, int batch_size) {\n    multi_head_attention(layer_id, batch_size);\n    \n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            attention_output[b][0][i] += layer_output[layer_id][b][i];\n        }\n    }\n    \n    layer_norm(batch_size, attention_output[0]);\n    feed_forward(layer_id, batch_size);\n    \n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            layer_output[layer_id+1][b][i] += layer_norm_buffer[b][i];\n        }\n    }\n}\n\n// ================== 主程序 ==================\nint main() {\n    // 初始化模型\n    init_parameters();\n    \n    // 准备输入数据\n    int input_ids[BATCH_SIZE][SEQ_LEN];\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int i = 0; i < SEQ_LEN; i++) {\n            input_ids[b][i] = my_rand() % VOCAB_SIZE;\n        }\n    }\n    \n    // 词嵌入查找\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int i = 0; i < SEQ_LEN; i++) {\n            memcpy(layer_output[0][b] + i*HIDDEN_SIZE, \n                  embeddings[input_ids[b][i]], \n                  HIDDEN_SIZE * sizeof(float));\n        }\n    }\n    \n    // 前向传播\n    for (int l = 0; l < NUM_LAYERS; l++) {\n        transformer_layer(l, BATCH_SIZE);\n    }\n    \n    // 输出结果\n   \n    \n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 884,\n        \"min_slack\": 1.1102230246251565e-15,\n        \"max_frequency\": 100.00000000000001,\n        \"estimated_resources_area\": 34913,\n        \"dsp_count\": 15,\n        \"flip_flops\": 15671\n    },\n    \"silicon_compiler\": {\n        \"area\": 1425740.0,\n        \"max_frequency\": 18.494988782789303,\n        \"power\": 237.642\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 128\n#define OUT_CHANNELS 128\n#define IN_HEIGHT 128\n#define IN_WIDTH 128\n#define POOL1_SIZE 32\n#define POOL2_SIZE 16\n#define POOL3_SIZE 8\n#define EPSILON 1e-5\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float pool1_output[BATCH_SIZE][OUT_CHANNELS / 4][POOL1_SIZE][POOL1_SIZE];\nstatic float pool2_output[BATCH_SIZE][OUT_CHANNELS / 4][POOL2_SIZE][POOL2_SIZE];\nstatic float pool3_output[BATCH_SIZE][OUT_CHANNELS / 4][POOL3_SIZE][POOL3_SIZE];\nstatic float pool4_output[BATCH_SIZE][OUT_CHANNELS / 4][IN_HEIGHT][IN_WIDTH];\nstatic float spp_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT / 2][IN_WIDTH / 2];\nstatic float conv1_output[BATCH_SIZE][OUT_CHANNELS / 2][IN_HEIGHT / 2][IN_WIDTH / 2];\nstatic float output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT / 2][IN_WIDTH / 2];\n\nstatic unsigned long _rand_seed = 1;\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\n\n// 4x4池化 + 1x1卷积 + 上采样\nvoid pool1() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 4; oc++) {\n            for (int h = 0; h < POOL1_SIZE; h++) {\n                for (int w = 0; w < POOL1_SIZE; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h * 4][w * 4] * 0.01;  // 假设权重为0.01\n                    }\n                    pool1_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 8x8池化 + 1x1卷积 + 上采样\nvoid pool2() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 4; oc++) {\n            for (int h = 0; h < POOL2_SIZE; h++) {\n                for (int w = 0; w < POOL2_SIZE; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h * 8][w * 8] * 0.01;  // 假设权重为0.01\n                    }\n                    pool2_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 16x16池化 + 1x1卷积 + 上采样\nvoid pool3() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 4; oc++) {\n            for (int h = 0; h < POOL3_SIZE; h++) {\n                for (int w = 0; w < POOL3_SIZE; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h * 16][w * 16] * 0.01;  // 假设权重为0.01\n                    }\n                    pool3_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 原始输入 + 1x1卷积\nvoid pool4() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 4; oc++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    pool4_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 多尺度拼接\nvoid spp_concat() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT / 2; h++) {\n                for (int w = 0; w < IN_WIDTH / 2; w++) {\n                    if (c < OUT_CHANNELS / 4) {\n                        spp_output[b][c][h][w] = pool1_output[b][c][h][w];\n                    } else if (c < OUT_CHANNELS / 2) {\n                        spp_output[b][c][h][w] = pool2_output[b][c - OUT_CHANNELS / 4][h][w];\n                    } else if (c < 3 * OUT_CHANNELS / 4) {\n                        spp_output[b][c][h][w] = pool3_output[b][c - OUT_CHANNELS / 2][h][w];\n                    } else {\n                        spp_output[b][c][h][w] = pool4_output[b][c - 3 * OUT_CHANNELS / 4][h][w];\n                    }\n                }\n            }\n        }\n    }\n}\n\n// 1x1卷积 (通道融合)\nvoid conv1() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 2; oc++) {\n            for (int h = 0; h < IN_HEIGHT / 2; h++) {\n                for (int w = 0; w < IN_WIDTH / 2; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < OUT_CHANNELS; ic++) {\n                        sum += spp_output[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    conv1_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 1x1卷积 (通道融合)\nvoid conv2() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS; oc++) {\n            for (int h = 0; h < IN_HEIGHT / 2; h++) {\n                for (int w = 0; w < IN_WIDTH / 2; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < OUT_CHANNELS / 2; ic++) {\n                        sum += conv1_output[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 打印输出\n\n\n// 主函数\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    pool1();      // 4x4池化\n    pool2();      // 8x8池化\n    pool3();      // 16x16池化\n    pool4();      // 原始输入\n    spp_concat(); // 多尺度拼接\n    conv1();      // 1x1卷积 (通道融合)\n    conv2();      // 1x1卷积 (通道融合)\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 242,\n        \"min_slack\": 0.04769998999996372,\n        \"max_frequency\": 100.47928609418965,\n        \"estimated_resources_area\": 15277,\n        \"dsp_count\": 5,\n        \"flip_flops\": 4505\n    },\n    \"silicon_compiler\": {\n        \"area\": 285727.0,\n        \"max_frequency\": 19.226147560682527,\n        \"power\": 44.8615\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 256\n#define OUT_CHANNELS 512\n#define IN_HEIGHT 64\n#define IN_WIDTH 64\n#define RATIO 16\n#define KERNEL_SIZE 7\n#define GROUPS 8\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float avg_pool_output[BATCH_SIZE][IN_CHANNELS][1][1];\nstatic float fc1_output[BATCH_SIZE][IN_CHANNELS / RATIO];\nstatic float fc2_output[BATCH_SIZE][IN_CHANNELS];\nstatic float channel_att_output[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float avg_spatial[BATCH_SIZE][1][IN_HEIGHT][IN_WIDTH];\nstatic float max_spatial[BATCH_SIZE][1][IN_HEIGHT][IN_WIDTH];\nstatic float spatial_att_output[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float cbam_output[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float fpn_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT * 2][IN_WIDTH * 2];\n\n\nstatic unsigned long _rand_seed = 1;\n\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\n\n// 全局平均池化\nvoid global_avg_pool() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            float sum = 0.0;\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    sum += input[b][c][h][w];\n                }\n            }\n            avg_pool_output[b][c][0][0] = sum / (IN_HEIGHT * IN_WIDTH);\n        }\n    }\n}\n\n// 全连接层 (通道压缩)\nvoid fc1() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < IN_CHANNELS / RATIO; oc++) {\n            float sum = 0.0;\n            for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                sum += avg_pool_output[b][ic][0][0] * 0.01;  // 假设权重为0.01\n            }\n            fc1_output[b][oc] = fmaxf(0.0, sum);  // ReLU\n        }\n    }\n}\n\n// 全连接层 (通道恢复)\nvoid fc2() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < IN_CHANNELS; oc++) {\n            float sum = 0.0;\n            for (int ic = 0; ic < IN_CHANNELS / RATIO; ic++) {\n                sum += fc1_output[b][ic] * 0.01;  // 假设权重为0.01\n            }\n            fc2_output[b][oc] = 1.0 / (1.0 + expf(-sum));  // Sigmoid\n        }\n    }\n}\n\n// 通道注意力\nvoid channel_attention() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    channel_att_output[b][c][h][w] = input[b][c][h][w] * fc2_output[b][c];\n                }\n            }\n        }\n    }\n}\n\n// 空间平均池化\nvoid spatial_avg_pool() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int h = 0; h < IN_HEIGHT; h++) {\n            for (int w = 0; w < IN_WIDTH; w++) {\n                float sum = 0.0;\n                for (int c = 0; c < IN_CHANNELS; c++) {\n                    sum += channel_att_output[b][c][h][w];\n                }\n                avg_spatial[b][0][h][w] = sum / IN_CHANNELS;\n            }\n        }\n    }\n}\n\n// 空间最大池化\nvoid spatial_max_pool() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int h = 0; h < IN_HEIGHT; h++) {\n            for (int w = 0; w < IN_WIDTH; w++) {\n                float max_val = -INFINITY;\n                for (int c = 0; c < IN_CHANNELS; c++) {\n                    max_val = fmaxf(max_val, channel_att_output[b][c][h][w]);\n                }\n                max_spatial[b][0][h][w] = max_val;\n            }\n        }\n    }\n}\n\n// 空间注意力 (7x7卷积)\nvoid spatial_attention() {\n    int padding = KERNEL_SIZE / 2;\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int h = 0; h < IN_HEIGHT; h++) {\n            for (int w = 0; w < IN_WIDTH; w++) {\n                float sum = 0.0;\n                for (int kh = 0; kh < KERNEL_SIZE; kh++) {\n                    for (int kw = 0; kw < KERNEL_SIZE; kw++) {\n                        int ih = h + kh - padding;\n                        int iw = w + kw - padding;\n                        if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                            sum += (avg_spatial[b][0][ih][iw] + max_spatial[b][0][ih][iw]) * 0.01;  // 假设权重为0.01\n                        }\n                    }\n                }\n                spatial_att_output[b][0][h][w] = 1.0 / (1.0 + expf(-sum));  // Sigmoid\n            }\n        }\n    }\n}\n\n// CBAM模块\nvoid cbam() {\n    channel_attention();\n    spatial_attention();\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    cbam_output[b][c][h][w] = channel_att_output[b][c][h][w] * spatial_att_output[b][0][h][w];\n                }\n            }\n        }\n    }\n}\n\n// FPN横向连接块 (分组卷积)\nvoid fpn_block(int in_channels, int out_channels, float (*input)[IN_CHANNELS][IN_HEIGHT][IN_WIDTH], float (*output)[OUT_CHANNELS][IN_HEIGHT * 2][IN_WIDTH * 2]) {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < out_channels; oc++) {\n            for (int h = 0; h < IN_HEIGHT * 2; h++) {\n                for (int w = 0; w < IN_WIDTH * 2; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < in_channels / GROUPS; ic++) {\n                        for (int kh = 0; kh < 3; kh++) {\n                            for (int kw = 0; kw < 3; kw++) {\n                                int ih = h / 2 + kh - 1;\n                                int iw = w / 2 + kw - 1;\n                                if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                    sum += input[b][ic][ih][iw] * 0.01;  // 假设权重为0.01\n                                }\n                            }\n                        }\n                    }\n                    output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 轻量化FPN\nvoid light_fpn() {\n    // CBAM处理\n    cbam();\n    \n    // FPN自上而下路径\n    fpn_block(512, 256, cbam_output, fpn_output);  // 假设高层特征输入为512通道\n}\n\n// 打印输出\n\n\n// 主函数\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    light_fpn();\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 250,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 22678,\n        \"dsp_count\": 3,\n        \"flip_flops\": 3774\n    },\n    \"silicon_compiler\": {\n        \"area\": 403389.0,\n        \"max_frequency\": 18.986428500907554,\n        \"power\": 116.263\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n#include <time.h>\n\n// 定义常量\n#define BATCH_SIZE 16\n#define IN_CHANNELS 3\n#define OUT_CHANNELS 3\n#define IMG_SIZE 64\n#define HR_IMG_SIZE 128//256//\n#define NUM_RESIDUAL_BLOCKS 4\n#define KERNEL_SIZE 3\n#define DILATION 2\n#define LEARNING_RATE 1e-4\n#define EPOCHS 100\n\n// 定义矩阵结构\ntypedef struct {\n    int rows, cols;\n    float *data;\n} Matrix;\n\n// 初始化矩阵\nMatrix create_matrix(int rows, int cols) {\n    Matrix mat;\n    mat.rows = rows;\n    mat.cols = cols;\n    mat.data = (float *)malloc(rows * cols * sizeof(float));\n    return mat;\n}\n\n// 释放矩阵内存\nvoid free_matrix(Matrix mat) {\n    free(mat.data);\n}\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat log(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\nstatic unsigned long _rand_seed = 1;\n\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n\n\n\n\n\n// 卷积操作\nMatrix conv2d(Matrix input, Matrix kernel, int stride, int padding, int dilation) {\n    int out_rows = (input.rows + 2 * padding - dilation * (kernel.rows - 1) - 1) / stride + 1;\n    int out_cols = (input.cols + 2 * padding - dilation * (kernel.cols - 1) - 1) / stride + 1;\n    Matrix output = create_matrix(out_rows, out_cols);\n\n    for (int i = 0; i < out_rows; i++) {\n        for (int j = 0; j < out_cols; j++) {\n            float sum = 0.0;\n            for (int ki = 0; ki < kernel.rows; ki++) {\n                for (int kj = 0; kj < kernel.cols; kj++) {\n                    int ii = i * stride + ki * dilation - padding;\n                    int jj = j * stride + kj * dilation - padding;\n                    if (ii >= 0 && ii < input.rows && jj >= 0 && jj < input.cols) {\n                        sum += input.data[ii * input.cols + jj] * kernel.data[ki * kernel.cols + kj];\n                    }\n                }\n            }\n            output.data[i * out_cols + j] = sum;\n        }\n    }\n    return output;\n}\n\n// 残差块\nMatrix residual_block(Matrix input) {\n    Matrix kernel = create_matrix(KERNEL_SIZE, KERNEL_SIZE);\n    for (int i = 0; i < KERNEL_SIZE * KERNEL_SIZE; i++) {\n        kernel.data[i] = (float)rand() / RAND_MAX;  // 随机初始化卷积核\n    }\n\n    Matrix conv1 = conv2d(input, kernel, 1, DILATION, DILATION);\n    Matrix conv2 = conv2d(conv1, kernel, 1, DILATION, DILATION);\n\n    for (int i = 0; i < input.rows * input.cols; i++) {\n        conv2.data[i] += input.data[i];  // 残差连接\n    }\n\n    free_matrix(kernel);\n    free_matrix(conv1);\n    return conv2;\n}\n\n// 生成器\nMatrix generator(Matrix input) {\n    Matrix output = input;\n    for (int i = 0; i < NUM_RESIDUAL_BLOCKS; i++) {\n        output = residual_block(output);\n    }\n    return output;\n}\n\n// 判别器\nMatrix discriminator(Matrix input) {\n    Matrix kernel = create_matrix(KERNEL_SIZE, KERNEL_SIZE);\n    for (int i = 0; i < KERNEL_SIZE * KERNEL_SIZE; i++) {\n        kernel.data[i] = (float)rand() / RAND_MAX;  // 随机初始化卷积核\n    }\n\n    Matrix conv1 = conv2d(input, kernel, 2, 1, 1);\n    Matrix conv2 = conv2d(conv1, kernel, 2, 1, 1);\n    Matrix conv3 = conv2d(conv2, kernel, 2, 1, 1);\n\n    free_matrix(kernel);\n    free_matrix(conv1);\n    free_matrix(conv2);\n    return conv3;\n}\n\n// 感知损失（简化版）\n// 感知损失（简化版）\ndouble perceptual_loss(Matrix hr_real, Matrix hr_fake) {\n    int min_rows = hr_real.rows < hr_fake.rows ? hr_real.rows : hr_fake.rows;\n    int min_cols = hr_real.cols < hr_fake.cols ? hr_real.cols : hr_fake.cols;\n\n    double loss = 0.0;\n    for (int i = 0; i < min_rows; i++) {\n        for (int j = 0; j < min_cols; j++) {\n            float diff = hr_real.data[i * hr_real.cols + j] - hr_fake.data[i * hr_fake.cols + j];\n            loss += pow(diff, 2);\n        }\n    }\n    return loss / (min_rows * min_cols);\n}\n\n// 对抗损失\nfloat adversarial_loss(Matrix pred, int is_real) {\n    float loss = 0.0;\n    for (int i = 0; i < pred.rows * pred.cols; i++) {\n        if (is_real) {\n            loss += -log(pred.data[i]);\n        } else {\n            loss += -log(1 - pred.data[i]);\n        }\n    }\n    return loss / (pred.rows * pred.cols);\n}\n\n// 训练函数\nvoid train(Matrix *lr_imgs, Matrix *hr_imgs, int num_images) {\n    for (int epoch = 0; epoch < EPOCHS; epoch++) {\n        for (int i = 0; i < num_images; i++) {\n            // 生成器前向传播\n            Matrix fake_hr = generator(lr_imgs[i]);\n            Matrix pred_fake = discriminator(fake_hr);\n\n            // 计算生成器损失\n            float loss_G_adv = adversarial_loss(pred_fake, 1);\n            float loss_G_perc = perceptual_loss(hr_imgs[i], fake_hr);\n            float loss_G = loss_G_adv + 0.006 * loss_G_perc;\n\n            // 判别器前向传播\n            Matrix pred_real = discriminator(hr_imgs[i]);\n            float loss_D_real = adversarial_loss(pred_real, 1);\n            float loss_D_fake = adversarial_loss(pred_fake, 0);\n            float loss_D = (loss_D_real + loss_D_fake) * 0.5;\n\n            // 打印训练日志\n           \n\n            // 释放内存\n            free_matrix(fake_hr);\n            free_matrix(pred_fake);\n            free_matrix(pred_real);\n        }\n    }\n}\n\n// 主函数\nint main() {\n   \n\n    // 模拟数据集\n    int num_images = 100;\n    Matrix lr_imgs[100];\n    Matrix hr_imgs[100];\n\n    for (int i = 0; i < 100 ; i++) {\n        lr_imgs[i] = create_matrix(IMG_SIZE, IMG_SIZE);\n        hr_imgs[i] = create_matrix(HR_IMG_SIZE, HR_IMG_SIZE);\n        for (int j = 0; j < IMG_SIZE * IMG_SIZE; j++) {\n            lr_imgs[i].data[j] = (float)rand() / RAND_MAX;  // 随机初始化低分辨率图像\n        }\n        for (int j = 0; j < HR_IMG_SIZE * HR_IMG_SIZE; j++) {\n            hr_imgs[i].data[j] = (float)rand() / RAND_MAX;  // 随机初始化高分辨率图像\n        }\n    }\n\n    // 训练模型\n    train(lr_imgs, hr_imgs, num_images);\n\n    // 释放数据集内存\n    for (int i = 0; i < num_images; i++) {\n        free_matrix(lr_imgs[i]);\n        free_matrix(hr_imgs[i]);\n    }\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 332,\n        \"min_slack\": 1.1102230246251565e-15,\n        \"max_frequency\": 100.00000000000001,\n        \"estimated_resources_area\": 20773,\n        \"dsp_count\": 57,\n        \"flip_flops\": 6821\n    },\n    \"silicon_compiler\": {\n        \"area\": 1357340.0,\n        \"max_frequency\": 19.079636494765502,\n        \"power\": 726.4000000000001\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n#include <string.h>\n\n// 输入输出参数\n#define IN_SIZE 128//512\n#define IN_CHANNELS 3\n#define OUT_CHANNELS 1\n#define NUM_ENCODER_STAGES 5  // ResNet-50的5个阶段特征\n\n// 编码器参数（简化ResNet-50配置）\nstatic const int encoder_channels[NUM_ENCODER_STAGES] = {64, 256, 512, 1024, 2048};\nstatic const int encoder_sizes[NUM_ENCODER_STAGES] = {256, 128, 64, 32, 16}; // 各阶段特征图尺寸\n\n// 解码器参数\n#define NUM_UPSAMPLES 4\nstatic const int decoder_channels[NUM_UPSAMPLES+1] = {512, 256, 128, 64, 32};\n\n// 特征存储结构体\ntypedef struct {\n    float* data;\n    int size;\n    int channels;\n} FeatureMap;\n\n// 全局特征存储\nFeatureMap encoder_features[NUM_ENCODER_STAGES];\nFeatureMap decoder_features[NUM_UPSAMPLES+1];\n\n\n\n\n\nfloat floorf(float x) {\n    // 将浮点数转换为无符号整型以进行位操作\n    unsigned int bits = *(unsigned int*)&x;\n    int sign = bits >> 31;                     // 符号位 (0:正数, 1:负数)\n    int exponent = (bits >> 23) & 0xFF;        // 指数部分\n    int mantissa = bits & 0x007FFFFF;          // 尾数部分 (23位)\n\n    // 处理特殊情况：NaN、无穷大、零\n    if (exponent == 0xFF) return x;            // NaN或Inf\n    if (exponent == 0 && mantissa == 0) return x; // ±0.0\n\n    // 计算实际指数（减去偏移量127）\n    int exp_val = exponent - 127;\n\n    // 情况1：x的绝对值 >= 1.0\n    if (exp_val >= 23) {                       // 没有小数位（尾数全为整数）\n        return x;\n    } else if (exp_val >= 0) {                 // 有整数和小数部分\n        // 计算小数部分的位数（从右数第(23 - exp_val)位开始为小数）\n        int fractional_bits = 23 - exp_val;\n        unsigned int truncate_mask = 0xFFFFFFFF << fractional_bits;\n        unsigned int truncated = mantissa & truncate_mask;\n\n        // 构造整数部分的位表示\n        unsigned int result_bits = (sign << 31) | ((exponent) << 23) | truncated;\n        float result = *(float*)&result_bits;\n\n        // 处理负数且有小数的情况（例如-2.3 -> -3.0）\n        if (sign && (mantissa != truncated)) {\n            result -= 1.0f;\n        }\n        return result;\n\n    // 情况2：x的绝对值 < 1.0 (exp_val < 0)\n    } else {                                   \n        return (sign) ? -1.0f : 0.0f;          // 正数取0，负数取-1\n    }\n}\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\n\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n/******************** 核心函数实现 ​********************/\n\n// 初始化特征图内存\nFeatureMap create_feature(int size, int channels) {\n    FeatureMap fm;\n    fm.size = size;\n    fm.channels = channels;\n    fm.data = (float*)malloc(size * size * channels * sizeof(float));\n    memset(fm.data, 0, size * size * channels * sizeof(float));\n    return fm;\n}\n\n// 简化版ResNet-50编码器（仅保留下采样逻辑）\nvoid simplified_resnet50(FeatureMap input) {\n    for (int stage = 0; stage < NUM_ENCODER_STAGES; stage++) {\n        int new_size = encoder_sizes[stage];\n        int ch = encoder_channels[stage];\n        \n        // 创建特征存储\n        encoder_features[stage] = create_feature(new_size, ch);\n        \n        // 模拟卷积和下采样操作（实际应包含残差块）\n        for (int i = 0; i < new_size; i++) {\n            for (int j = 0; j < new_size; j++) {\n                for (int c = 0; c < ch; c++) {\n                    // 模拟卷积核计算（实际应使用预训练权重）\n                    float sum = 0.0f;\n                    for (int di = 0; di < 2; di++) {\n                        for (int dj = 0; dj < 2; dj++) {\n                            int input_i = i * 2 + di;\n                            int input_j = j * 2 + dj;\n                            if (input_i < input.size && input_j < input.size) {\n                                sum += input.data[(input_i * input.size + input_j) * input.channels + c % input.channels];\n                            }\n                        }\n                    }\n                    encoder_features[stage].data[(i * new_size + j) * ch + c] = sum * 0.25f; // 平均池化\n                }\n            }\n        }\n        \n        input = encoder_features[stage]; // 更新输入到下一阶段\n    }\n}\n\n// 双线性上采样\nFeatureMap upsample_bilinear(FeatureMap input) {\n    int new_size = input.size * 2;\n    FeatureMap output = create_feature(new_size, input.channels);\n    \n    for (int i = 0; i < new_size; i++) {\n        float fi = (float)i / new_size * input.size;\n        int i0 = (int)floor(fi);\n        int i1 = i0 + 1 < input.size ? i0 + 1 : i0;\n        float di = fi - i0;\n        \n        for (int j = 0; j < new_size; j++) {\n            float fj = (float)j / new_size * input.size;\n            int j0 = (int)floor(fj);\n            int j1 = j0 + 1 < input.size ? j0 + 1 : j0;\n            float dj = fj - j0;\n            \n            for (int c = 0; c < input.channels; c++) {\n                // 双线性插值\n                float v00 = input.data[(i0 * input.size + j0) * input.channels + c];\n                float v01 = input.data[(i0 * input.size + j1) * input.channels + c];\n                float v10 = input.data[(i1 * input.size + j0) * input.channels + c];\n                float v11 = input.data[(i1 * input.size + j1) * input.channels + c];\n                \n                output.data[(i * new_size + j) * output.channels + c] = \n                    v00 * (1 - di) * (1 - dj) +\n                    v01 * (1 - di) * dj +\n                    v10 * di * (1 - dj) +\n                    v11 * di * dj;\n            }\n        }\n    }\n    return output;\n}\n\n// 带跳跃连接的解码器\nFeatureMap decoder_with_skips() {\n    // 初始化解码器输入（编码器最后层特征）\n    FeatureMap current = encoder_features[NUM_ENCODER_STAGES-1];\n    \n    for (int i = 0; i < NUM_UPSAMPLES; i++) {\n        // 上采样\n        current = upsample_bilinear(current);\n        \n        // 获取对应的编码器特征\n        int encoder_stage = NUM_ENCODER_STAGES - 2 - i;\n        FeatureMap skip = encoder_features[encoder_stage];\n        \n        // 调整编码器特征尺寸（如果需要）\n        if (skip.size != current.size) {\n            skip = upsample_bilinear(skip);\n        }\n        \n        // 特征拼接\n        FeatureMap concated = create_feature(current.size, current.channels + skip.channels);\n        memcpy(concated.data, current.data, current.size * current.size * current.channels * sizeof(float));\n        memcpy(concated.data + current.size * current.size * current.channels,\n               skip.data, skip.size * skip.size * skip.channels * sizeof(float));\n        \n        // 3x3卷积（简化实现）\n        FeatureMap conv_out = create_feature(concated.size, decoder_channels[i+1]);\n        for (int h = 0; h < concated.size; h++) {\n            for (int w = 0; w < concated.size; w++) {\n                for (int oc = 0; oc < decoder_channels[i+1]; oc++) {\n                    float sum = 0.0f;\n                    for (int di = -1; di <= 1; di++) {\n                        for (int dj = -1; dj <= 1; dj++) {\n                            int nh = h + di;\n                            int nw = w + dj;\n                            if (nh >= 0 && nh < concated.size && nw >= 0 && nw < concated.size) {\n                                for (int ic = 0; ic < concated.channels; ic++) {\n                                    sum += concated.data[(nh * concated.size + nw) * concated.channels + ic] * \n                                          0.01f; // 模拟卷积核权重\n                                }\n                            }\n                        }\n                    }\n                    conv_out.data[(h * conv_out.size + w) * conv_out.channels + oc] = fmaxf(sum, 0.0f); // ReLU\n                }\n            }\n        }\n        \n        current = conv_out;\n    }\n    \n    // 最终输出层\n    FeatureMap output = create_feature(IN_SIZE, OUT_CHANNELS);\n    for (int i = 0; i < IN_SIZE; i++) {\n        for (int j = 0; j < IN_SIZE; j++) {\n            float sum = 0.0f;\n            for (int c = 0; c < current.channels; c++) {\n                sum += current.data[(i * current.size + j) * current.channels + c] * 0.1f; // 模拟权重\n            }\n            output.data[i * IN_SIZE + j] = 1.0f / (1.0f + expf(-sum)); // Sigmoid\n        }\n    }\n    return output;\n}\n\n/******************** 主程序 ​********************/\nint main() {\n    // 创建输入特征\n    FeatureMap input = create_feature(IN_SIZE, IN_CHANNELS);\n    for (int i = 0; i < IN_SIZE * IN_SIZE * IN_CHANNELS; i++) {\n        input.data[i] = 0.5f; // 示例输入\n    }\n\n    // 编码器前向传播\n    simplified_resnet50(input);\n    \n    // 解码器前向传播\n    FeatureMap output = decoder_with_skips();\n\n    // 验证输出尺寸\n   \n    // 释放内存\n    for (int i = 0; i < NUM_ENCODER_STAGES; i++) {\n        free(encoder_features[i].data);\n    }\n    free(input.data);\n    free(output.data);\n    \n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 182,\n        \"min_slack\": 1.1102230246251565e-15,\n        \"max_frequency\": 100.00000000000001,\n        \"estimated_resources_area\": 29403,\n        \"dsp_count\": 48,\n        \"flip_flops\": 4879\n    },\n    \"silicon_compiler\": {\n        \"area\": 1536680.0,\n        \"max_frequency\": 17.32345665324676,\n        \"power\": 65.1152\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n\n// 参数定义\n#define BATCH_SIZE 2\n#define RADAR_POINTS 1024\n#define RADAR_DIM 4\n#define IMAGE_CHANNELS 3\n#define IMAGE_SIZE 224\n#define FEATURE_DIM 512\n#define NUM_HEADS 8\n#define HEAD_DIM (FEATURE_DIM / NUM_HEADS)\n\n// 全局缓冲区（静态内存）\nfloat radar_input[BATCH_SIZE][RADAR_POINTS][RADAR_DIM];   // 雷达输入\nfloat image_input[BATCH_SIZE][IMAGE_CHANNELS][IMAGE_SIZE][IMAGE_SIZE]; // 图像输入\nfloat radar_features[BATCH_SIZE][RADAR_POINTS][FEATURE_DIM]; // 雷达特征\nfloat img_features[BATCH_SIZE][9];                        // 图像特征\nfloat fused_features[BATCH_SIZE][FEATURE_DIM];            // 融合特征\nfloat heatmap[BATCH_SIZE];                                // 热图输出\nfloat bbox[BATCH_SIZE][7];                                // 边界框输出\n\n\nfloat exp(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n// 雷达编码器（4层MLP）\nvoid radar_encoder() {\n    // 模拟4层MLP（简化实现）\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int p = 0; p < RADAR_POINTS; p++) {\n            for (int c = 0; c < FEATURE_DIM; c++) {\n                radar_features[b][p][c] = 0.0f;\n                for (int i = 0; i < RADAR_DIM; i++) {\n                    radar_features[b][p][c] += radar_input[b][p][i] * 0.01f; // 模拟权重\n                }\n            }\n        }\n    }\n}\n\n// 图像编码器（简化ResNet-50）\nvoid image_encoder() {\n    // 模拟ResNet-50特征提取\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int i = 0; i < 9; i++) {\n            img_features[b][i] = 0.0f;\n            for (int c = 0; c < IMAGE_CHANNELS; c++) {\n                for (int h = 0; h < IMAGE_SIZE; h++) {\n                    for (int w = 0; w < IMAGE_SIZE; w++) {\n                        img_features[b][i] += image_input[b][c][h][w] * 0.001f; // 模拟权重\n                    }\n                }\n            }\n        }\n    }\n}\n\n// 可变形交叉注意力（简化实现）\nvoid deformable_cross_attention() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int p = 0; p < RADAR_POINTS; p++) {\n            for (int c = 0; c < FEATURE_DIM; c++) {\n                float query = 0.0f, value = 0.0f;\n                for (int i = 0; i < FEATURE_DIM; i++) {\n                    query += radar_features[b][p][i] * 0.01f; // 模拟Q权重\n                    value += img_features[b][i % 9] * 0.01f;   // 模拟V权重\n                }\n                float attention = exp(query * value / sqrt(FEATURE_DIM));\n                fused_features[b][c] += attention * value;     // 上下文聚合\n            }\n        }\n    }\n}\n\n// 检测头（热图 + 回归）\nvoid detection_head() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        heatmap[b] = 0.0f;\n        for (int c = 0; c < FEATURE_DIM; c++) {\n            heatmap[b] += fused_features[b][c] * 0.01f; // 模拟热图权重\n        }\n        heatmap[b] = 1.0f / (1.0f + exp(-heatmap[b])); // Sigmoid激活\n\n        for (int i = 0; i < 7; i++) {\n            bbox[b][i] = 0.0f;\n            for (int c = 0; c < FEATURE_DIM; c++) {\n                bbox[b][i] += fused_features[b][c] * 0.01f; // 模拟回归权重\n            }\n        }\n    }\n}\n\n// 主函数\nint main() {\n    // 初始化输入数据（模拟）\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int p = 0; p < RADAR_POINTS; p++) {\n            for (int i = 0; i < RADAR_DIM; i++) {\n                radar_input[b][p][i] = 0.5f; // 示例雷达数据\n            }\n        }\n        for (int c = 0; c < IMAGE_CHANNELS; c++) {\n            for (int h = 0; h < IMAGE_SIZE; h++) {\n                for (int w = 0; w < IMAGE_SIZE; w++) {\n                    image_input[b][c][h][w] = 0.5f; // 示例图像数据\n                }\n            }\n        }\n    }\n\n    // 执行推理\n    radar_encoder();                    // 雷达编码\n    image_encoder();                    // 图像编码\n    deformable_cross_attention();       // BEV融合\n    detection_head();                   // 检测头\n\n  \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 316,\n        \"min_slack\": 5e-10,\n        \"max_frequency\": 100.000000005,\n        \"estimated_resources_area\": 22595,\n        \"dsp_count\": 8,\n        \"flip_flops\": 7218\n    },\n    \"silicon_compiler\": {\n        \"area\": 590003.0,\n        \"max_frequency\": 16.10194463185319,\n        \"power\": 278.553\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 512\n#define OUT_CHANNELS 512\n#define IN_HEIGHT 32\n#define IN_WIDTH 32\n#define RATIO 16\n#define KERNEL_SIZE 7\n#define GROUPS 8\n#define NUM_RATIOS 9\n#define BASE_SIZE 32\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float loc_map[BATCH_SIZE][1][IN_HEIGHT][IN_WIDTH];\nstatic float anchors[BATCH_SIZE][NUM_RATIOS][2];  // 9种比例的宽高\nstatic float aligned_feat[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float fpn_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT * 2][IN_WIDTH * 2];\n\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\nstatic unsigned long _rand_seed = 1;\n\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\n\n// 动态Anchor生成\nvoid guided_anchoring() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    // 位置概率图\n                    loc_map[b][0][h][w] = 1.0 / (1.0 + expf(-input[b][c][h][w]));  // Sigmoid\n                }\n            }\n        }\n    }\n\n    // 生成9种比例的宽高\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int i = 0; i < NUM_RATIOS; i++) {\n            anchors[b][i][0] = BASE_SIZE * expf(0.01 * input[b][i * 2][0][0]);      // 宽\n            anchors[b][i][1] = BASE_SIZE * expf(0.01 * input[b][i * 2 + 1][0][0]);   // 高\n        }\n    }\n}\n\n// 改进型RoI Align\nvoid enhanced_roi_align() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    // 双线性插值\n                    float sum = 0.0;\n                    for (int kh = 0; kh < KERNEL_SIZE; kh++) {\n                        for (int kw = 0; kw < KERNEL_SIZE; kw++) {\n                            int ih = h + kh - KERNEL_SIZE / 2;\n                            int iw = w + kw - KERNEL_SIZE / 2;\n                            if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                sum += input[b][c][ih][iw] * 0.01;  // 假设权重为0.01\n                            }\n                        }\n                    }\n                    aligned_feat[b][c][h][w] = sum / (KERNEL_SIZE * KERNEL_SIZE);\n                }\n            }\n        }\n    }\n}\n\n// 多级FPN特征裁剪\nvoid multi_fpn_extract() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT * 2; h++) {\n                for (int w = 0; w < IN_WIDTH * 2; w++) {\n                    // 特征融合\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS / GROUPS; ic++) {\n                        for (int kh = 0; kh < 3; kh++) {\n                            for (int kw = 0; kw < 3; kw++) {\n                                int ih = h / 2 + kh - 1;\n                                int iw = w / 2 + kw - 1;\n                                if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                    sum += aligned_feat[b][ic][ih][iw] * 0.01;  // 假设权重为0.01\n                                }\n                            }\n                        }\n                    }\n                    fpn_output[b][c][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 主函数\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 动态Anchor生成\n    guided_anchoring();\n\n    // 改进型RoI Align\n    enhanced_roi_align();\n\n    // 多级FPN特征裁剪\n    multi_fpn_extract();\n\n    // 打印输出\n    \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 14,\n        \"min_slack\": 1.511000000000004,\n        \"max_frequency\": 117.79950524207804,\n        \"estimated_resources_area\": 1228,\n        \"dsp_count\": 3,\n        \"flip_flops\": 377\n    },\n    \"silicon_compiler\": {\n        \"area\": 29689.7,\n        \"max_frequency\": 27.89563684344132,\n        \"power\": 2.17652\n    }\n}"
    }
,
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 64\n#define OUT_CHANNELS 128\n#define IN_HEIGHT 128\n#define IN_WIDTH 128\n#define KERNEL_SIZE 3\n#define EPSILON 1e-5\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float conv1_output[BATCH_SIZE][OUT_CHANNELS / 2][IN_HEIGHT][IN_WIDTH];\nstatic float depthwise_output[BATCH_SIZE][OUT_CHANNELS / 2][IN_HEIGHT][IN_WIDTH];\nstatic float conv2_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float branch2_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\n\nstatic unsigned long _rand_seed = 1;\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n\n\n\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\n\n// 1x1卷积 (降维)\nvoid conv1x1_reduce() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 2; oc++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    conv1_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 深度可分离卷积\nvoid depthwise_conv() {\n    int padding = 1;\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS / 2; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int kh = 0; kh < KERNEL_SIZE; kh++) {\n                        for (int kw = 0; kw < KERNEL_SIZE; kw++) {\n                            int ih = h + kh - padding;\n                            int iw = w + kw - padding;\n                            if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                sum += conv1_output[b][c][ih][iw] * 0.01;  // 假设权重为0.01\n                            }\n                        }\n                    }\n                    depthwise_output[b][c][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 1x1卷积 (升维)\nvoid conv1x1_expand() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS; oc++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < OUT_CHANNELS / 2; ic++) {\n                        sum += depthwise_output[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    conv2_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// Branch2: 1x1卷积\nvoid branch2_conv() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS; oc++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    branch2_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// Skip Connection: 逐元素相加 + ReLU\nvoid skip_connection() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = conv2_output[b][c][h][w] + branch2_output[b][c][h][w];\n                    output[b][c][h][w] = fmaxf(0.0, sum);  // ReLU\n                }\n            }\n        }\n    }\n}\n\n// 打印输出\n\n\n// 主函数\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    conv1x1_reduce();      // Branch1: 1x1 Conv (降维)\n    depthwise_conv();      // Branch1: Depthwise Conv\n    conv1x1_expand();     // Branch1: 1x1 Conv (升维)\n    branch2_conv();       // Branch2: 1x1 Conv\n    skip_connection();    // Skip Connection: Add + ReLU\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 204,\n        \"min_slack\": 0.04769998999996372,\n        \"max_frequency\": 100.47928609418965,\n        \"estimated_resources_area\": 13057,\n        \"dsp_count\": 5,\n        \"flip_flops\": 2916\n    },\n    \"silicon_compiler\": {\n        \"area\": 235087.0,\n        \"max_frequency\": 20.870073358307852,\n        \"power\": 18.089399999999998\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n//#include <math.h>\n#include <string.h>\n\n// 输入输出参数\n#define BATCH_SIZE 2\n#define IN_CHANNELS 256\n#define OUT_CHANNELS 512\n#define IN_SIZE 64\n#define NUM_HEADS 8\n#define HEAD_DIM (OUT_CHANNELS / NUM_HEADS)\n\n// 全局静态数组（预分配内存）\nfloat input_feature[BATCH_SIZE][IN_CHANNELS][IN_SIZE][IN_SIZE]; // 输入特征\nfloat output_feature[BATCH_SIZE][OUT_CHANNELS][IN_SIZE][IN_SIZE]; // 输出特征\nfloat dilated_output[BATCH_SIZE][OUT_CHANNELS][IN_SIZE][IN_SIZE]; // 空洞卷积输出\nfloat attention_output[BATCH_SIZE][OUT_CHANNELS][IN_SIZE][IN_SIZE]; // 注意力输出\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\n\nfloat exp(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n\n\n\n\n// 空洞卷积层\nvoid dilated_conv(int in_channels, int out_channels, int kernel_size, int dilation) {\n    int padding = dilation * (kernel_size - 1) / 2;\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < out_channels; oc++) {\n            for (int h = 0; h < IN_SIZE; h++) {\n                for (int w = 0; w < IN_SIZE; w++) {\n                    float sum = 0.0f;\n                    for (int ic = 0; ic < in_channels; ic++) {\n                        for (int kh = 0; kh < kernel_size; kh++) {\n                            for (int kw = 0; kw < kernel_size; kw++) {\n                                int nh = h + kh * dilation - padding;\n                                int nw = w + kw * dilation - padding;\n                                if (nh >= 0 && nh < IN_SIZE && nw >= 0 && nw < IN_SIZE) {\n                                    sum += input_feature[b][ic][nh][nw] * 0.01f; // 模拟卷积核权重\n                                }\n                            }\n                        }\n                    }\n                    dilated_output[b][oc][h][w] = fmaxf(sum, 0.0f); // ReLU\n                }\n            }\n        }\n    }\n}\n\n// 多头注意力机制\nvoid multi_head_attention(int num_heads, int head_dim) {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int h = 0; h < IN_SIZE; h++) {\n            for (int w = 0; w < IN_SIZE; w++) {\n                for (int head = 0; head < num_heads; head++) {\n                    float query = 0.0f, key = 0.0f, value = 0.0f;\n                    for (int c = 0; c < head_dim; c++) {\n                        query += dilated_output[b][head * head_dim + c][h][w] * 0.01f; // 模拟Q权重\n                        key += dilated_output[b][head * head_dim + c][h][w] * 0.01f;   // 模拟K权重\n                        value += dilated_output[b][head * head_dim + c][h][w] * 0.01f; // 模拟V权重\n                    }\n                    float attention = exp(query * key / sqrt(head_dim)); // 注意力得分\n                    attention_output[b][head * head_dim][h][w] = attention * value; // 上下文聚合\n                }\n            }\n        }\n    }\n}\n\n// 全局上下文融合\nvoid global_context_fusion() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            float channel_sum = 0.0f;\n            for (int h = 0; h < IN_SIZE; h++) {\n                for (int w = 0; w < IN_SIZE; w++) {\n                    channel_sum += attention_output[b][c][h][w];\n                }\n            }\n            float channel_attn = 1.0f / (1.0f + exp(-channel_sum / (IN_SIZE * IN_SIZE))); // 通道注意力\n            for (int h = 0; h < IN_SIZE; h++) {\n                for (int w = 0; w < IN_SIZE; w++) {\n                    output_feature[b][c][h][w] = attention_output[b][c][h][w] * channel_attn; // 通道重标定\n                }\n            }\n        }\n    }\n}\n\n// 主函数\nint main() {\n    // 模拟输入数据\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_SIZE; h++) {\n                for (int w = 0; w < IN_SIZE; w++) {\n                    input_feature[b][c][h][w] = 0.5f; // 示例输入\n                }\n            }\n        }\n    }\n\n    // 多分支空洞卷积\n    dilated_conv(IN_CHANNELS, OUT_CHANNELS, 3, 2);\n\n    // 多头注意力机制\n    multi_head_attention(NUM_HEADS, HEAD_DIM);\n\n    // 全局上下文融合\n    global_context_fusion();\n\n    // 打印输出结果\n  \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 202,\n        \"min_slack\": 5e-10,\n        \"max_frequency\": 100.000000005,\n        \"estimated_resources_area\": 11441,\n        \"dsp_count\": 0,\n        \"flip_flops\": 3975\n    },\n    \"silicon_compiler\": {\n        \"area\": 291655.0,\n        \"max_frequency\": 21.202387388819982,\n        \"power\": 23.9133\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 3\n#define OUT_CHANNELS 64\n#define IN_HEIGHT 256\n#define IN_WIDTH 256\n#define KERNEL_SIZE 3\n#define POOL_SIZE 2\n#define POOL_STRIDE 2\n#define EPSILON 1e-5\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float conv_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float bn_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float relu_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float pool_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT / POOL_STRIDE][IN_WIDTH / POOL_STRIDE];\n#define INFINITY (1.0f / 0.0f)  // 正无穷大（float 类型）\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat pow(float base, float exponent) {\n    // 验证指数必须为整数且为正数\n    if (exponent <= 0.0f) return 0.0f / 0.0f; // NaN\n    \n    // 检查是否为整数\n    unsigned int exp_int = (unsigned int)exponent;\n    if ((float)exp_int != exponent) return 0.0f / 0.0f;\n\n    // 处理特殊情况\n    if (base == 0.0f) return 0.0f;\n\n    // 负数底数处理\n    int sign = 1;\n    if (base < 0.0f) {\n        sign = (exp_int % 2 == 1) ? -1 : 1;\n        base = -base;\n    }\n\n    // 快速幂计算\n    float result = 1.0f;\n    while (exp_int > 0) {\n        if (exp_int & 1) result *= base;\n        base *= base;\n        exp_int >>= 1;\n    }\n    return sign * result;\n}\n\n\n\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = 0.0f;//(float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\nfloat sqrt(float x) {\n    if (x <= 0) return 0.0f;\n    float guess = x / 2.0f;\n    for (int i = 0; i < 10; i++) {\n        guess = 0.5f * (guess + x / guess);\n    }\n    return guess;\n}\n\n// Z-Score归一化\nvoid z_score_normalization() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            // 计算均值和标准差\n            float mean = 0.0, std = 0.0;\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    mean += input[b][c][h][w];\n                }\n            }\n            mean /= (IN_HEIGHT * IN_WIDTH);\n\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    std += pow(input[b][c][h][w] - mean, 2);\n                }\n            }\n            std = sqrt(std / (IN_HEIGHT * IN_WIDTH) + EPSILON);\n\n            // 归一化\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (input[b][c][h][w] - mean) / std;\n                }\n            }\n        }\n    }\n}\n\n// 卷积层 (3x3, stride=1, padding=1)\nvoid conv2d() {\n    int padding = 1;\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS; oc++) {\n            for (int oh = 0; oh < IN_HEIGHT; oh++) {\n                for (int ow = 0; ow < IN_WIDTH; ow++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        for (int kh = 0; kh < KERNEL_SIZE; kh++) {\n                            for (int kw = 0; kw < KERNEL_SIZE; kw++) {\n                                int ih = oh + kh - padding;\n                                int iw = ow + kw - padding;\n                                if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                    sum += input[b][ic][ih][iw] * 0.01;  // 假设权重为0.01\n                                }\n                            }\n                        }\n                    }\n                    conv_output[b][oc][oh][ow] = sum;\n                }\n            }\n        }\n    }\n}\n\n// BatchNorm层\nvoid batch_norm() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            // 计算均值和方差\n            float mean = 0.0, var = 0.0;\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    mean += conv_output[b][c][h][w];\n                }\n            }\n            mean /= (IN_HEIGHT * IN_WIDTH);\n\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    var += pow(conv_output[b][c][h][w] - mean, 2);\n                }\n            }\n            var /= (IN_HEIGHT * IN_WIDTH);\n\n            // 归一化\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    bn_output[b][c][h][w] = (conv_output[b][c][h][w] - mean) / sqrt(var + EPSILON);\n                }\n            }\n        }\n    }\n}\n\n// ReLU激活\nvoid relu() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    relu_output[b][c][h][w] = fmaxf(0.0, bn_output[b][c][h][w]);\n                }\n            }\n        }\n    }\n}\n\n// MaxPool层 (2x2, stride=2)\nvoid max_pool() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int ph = 0; ph < IN_HEIGHT / POOL_STRIDE; ph++) {\n                for (int pw = 0; pw < IN_WIDTH / POOL_STRIDE; pw++) {\n                    float max_val = -INFINITY;\n                    for (int kh = 0; kh < POOL_SIZE; kh++) {\n                        for (int kw = 0; kw < POOL_SIZE; kw++) {\n                            int h = ph * POOL_STRIDE + kh;\n                            int w = pw * POOL_STRIDE + kw;\n                            if (h < IN_HEIGHT && w < IN_WIDTH) {\n                                max_val = fmaxf(max_val, relu_output[b][c][h][w]);\n                            }\n                        }\n                    }\n                    pool_output[b][c][ph][pw] = max_val;\n                }\n            }\n        }\n    }\n}\n\n// 打印输出\n// void print_output() {\n//     //printf(\"Pooling 输出尺寸: %dx%dx%d\\n\", \n//     //       IN_HEIGHT / POOL_STRIDE, IN_WIDTH / POOL_STRIDE, OUT_CHANNELS);\n//     //printf(\"Pooling 输出示例:\\n\");\n//     for (int c = 0; c < 3; c++) {\n//       //  printf(\"通道 %d:\\n\", c);\n//         for (int h = 0; h < 3; h++) {\n//             for (int w = 0; w < 3; w++) {\n//         //        printf(\"%.4f \", pool_output[0][c][h][w]);\n//             }\n//           //  printf(\"\\n\");\n//         }\n//     }\n// }\n\n// 主函数\nint _1_cnn() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    z_score_normalization();\n    conv2d();\n    batch_norm();\n    relu();\n    max_pool();\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    z_score_normalization();\n    conv2d();\n    batch_norm();\n    relu();\n    max_pool();\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}\n",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 371,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 19978,\n        \"dsp_count\": 0,\n        \"flip_flops\": 5374\n    },\n    \"silicon_compiler\": {\n        \"area\": 418359.0,\n        \"max_frequency\": 19.28368812105528,\n        \"power\": 36.86409999999999\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <string.h>\n\n// 定义常量\n#define VOCAB_SIZE 22        // BERT 词表大小30522\n#define HIDDEN_SIZE 68       // 768隐藏层维度\n#define N_LAYERS 12           // Transformer 层数\n#define N_HEADS 12            // 多头注意力头数\n#define FFN_HIDDEN_SIZE (HIDDEN_SIZE * 4)  // 前馈网络隐藏层大小\n#define MAX_SEQ_LEN 12       //512 最大序列长度\n#define DROPOUT 0.1           // Dropout 概率\n#define DIM HIDDEN_SIZE       // 定义 dim 为 HIDDEN_SIZE\n#define HEAD_DIM (DIM / N_HEADS)  // 每个注意力头的维度\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\n\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n\n\n// 定义矩阵乘法\nvoid matmul(float* a, float* b, float* output, int m, int n, int k) {\n    for (int i = 0; i < m; i++) {\n        for (int j = 0; j < k; j++) {\n            output[i * k + j] = 0.0f;\n            for (int l = 0; l < n; l++) {\n                output[i * k + j] += a[i * n + l] * b[l * k + j];\n            }\n        }\n    }\n}\n\n// 定义 Softmax\nvoid softmax(float* x, int size) {\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) max_val = x[i];\n    }\n\n    float sum_exp = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum_exp += x[i];\n    }\n\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum_exp;\n    }\n}\n\n// 定义 Layer Normalization\nvoid layer_norm(float* x, float* gamma, float* beta, int size) {\n    float mean = 0.0f, var = 0.0f;\n    for (int i = 0; i < size; i++) {\n        mean += x[i];\n    }\n    mean /= size;\n\n    for (int i = 0; i < size; i++) {\n        var += (x[i] - mean) * (x[i] - mean);\n    }\n    var /= size;\n\n    float std = sqrtf(var + 1e-5);\n    for (int i = 0; i < size; i++) {\n        x[i] = (x[i] - mean) / std * gamma[i] + beta[i];\n    }\n}\n\n// 定义多头注意力机制\nvoid multi_head_attention(float* query, float* key, float* value, float* output) {\n    float scores[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n    float attn_weights[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n\n    // 计算注意力分数\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        for (int j = 0; j < MAX_SEQ_LEN; j++) {\n            scores[i][j] = 0.0f;\n            for (int l = 0; l < HEAD_DIM; l++) {\n                scores[i][j] += query[i * HEAD_DIM + l] * key[j * HEAD_DIM + l];\n            }\n            scores[i][j] /= sqrtf(HEAD_DIM);\n        }\n    }\n\n    // Softmax\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        softmax(scores[i], MAX_SEQ_LEN);\n    }\n\n    // 加权求和\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        for (int j = 0; j < HEAD_DIM; j++) {\n            output[i * HEAD_DIM + j] = 0.0f;\n            for (int l = 0; l < MAX_SEQ_LEN; l++) {\n                output[i * HEAD_DIM + j] += scores[i][l] * value[l * HEAD_DIM + j];\n            }\n        }\n    }\n}\n\n// 定义前馈神经网络\nvoid feed_forward(float* x, float* w1, float* w2, float* output) {\n    float h[FFN_HIDDEN_SIZE];\n    matmul(x, w1, h, 1, DIM, FFN_HIDDEN_SIZE);\n    for (int i = 0; i < FFN_HIDDEN_SIZE; i++) {\n        h[i] = fmaxf(0.0f, h[i]);  // ReLU\n    }\n    matmul(h, w2, output, 1, FFN_HIDDEN_SIZE, DIM);\n}\n\n// 定义 Transformer Block\nvoid transformer_block(float* x, float* attn_weights, float* ffn_weights) {\n    float attn_output[DIM];\n    multi_head_attention(x, x, x, attn_output);\n\n    for (int i = 0; i < DIM; i++) {\n        x[i] += attn_output[i];  // 残差连接\n    }\n\n    float ffn_output[DIM];\n    feed_forward(x, ffn_weights, ffn_weights + DIM * FFN_HIDDEN_SIZE, ffn_output);\n\n    for (int i = 0; i < DIM; i++) {\n        x[i] += ffn_output[i];  // 残差连接\n    }\n}\n\n// 定义 BERT 模型\nvoid bert(float* input, float* embedding_weights, float* transformer_weights, float* output) {\n    float x[DIM];\n    matmul(input, embedding_weights, x, 1, MAX_SEQ_LEN, DIM);\n\n    for (int i = 0; i < N_LAYERS; i++) {\n        transformer_block(x, transformer_weights, transformer_weights + DIM * DIM);\n    }\n\n    // 将最终输出复制到 output\n    memcpy(output, x, DIM * sizeof(float));\n}\n\n// 定义下一句预测（NSP）\nvoid next_sentence_prediction(float* cls_output, float* nsp_weights, float* nsp_bias, float* output) {\n    float logits[2];\n    matmul(cls_output, nsp_weights, logits, 1, HIDDEN_SIZE, 2);\n    for (int i = 0; i < 2; i++) {\n        logits[i] += nsp_bias[i];\n    }\n    softmax(logits, 2);\n    memcpy(output, logits, 2 * sizeof(float));\n}\n\n// 定义掩码语言模型（MLM）\nvoid masked_language_model(float* output, float* mlm_weights, float* mlm_bias, float* mlm_output) {\n    float logits[MAX_SEQ_LEN * VOCAB_SIZE];\n    matmul(output, mlm_weights, logits, MAX_SEQ_LEN, HIDDEN_SIZE, VOCAB_SIZE);\n    for (int i = 0; i < MAX_SEQ_LEN * VOCAB_SIZE; i++) {\n        logits[i] += mlm_bias[i % VOCAB_SIZE];\n    }\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        softmax(logits + i * VOCAB_SIZE, VOCAB_SIZE);\n    }\n    memcpy(mlm_output, logits, MAX_SEQ_LEN * VOCAB_SIZE * sizeof(float));\n}\n\nint main() {\n    // 初始化输入和权重\n    float input[MAX_SEQ_LEN] = {0};\n    float embedding_weights[VOCAB_SIZE * HIDDEN_SIZE] = {0};\n    float transformer_weights[N_LAYERS * HIDDEN_SIZE * HIDDEN_SIZE * 2] = {0};\n    float nsp_weights[HIDDEN_SIZE * 2] = {0};\n    float nsp_bias[2] = {0};\n    float mlm_weights[HIDDEN_SIZE * VOCAB_SIZE] = {0};\n    float mlm_bias[VOCAB_SIZE] = {0};\n\n    // BERT 输出\n    float bert_output[HIDDEN_SIZE];\n    bert(input, embedding_weights, transformer_weights, bert_output);\n\n    // 下一句预测\n    float nsp_output[2];\n    next_sentence_prediction(bert_output, nsp_weights, nsp_bias, nsp_output);\n\n    // 掩码语言模型\n    float mlm_output[MAX_SEQ_LEN * VOCAB_SIZE];\n    masked_language_model(bert_output, mlm_weights, mlm_bias, mlm_output);\n\n  \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 630,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 31976,\n        \"dsp_count\": 0,\n        \"flip_flops\": 18229\n    },\n    \"silicon_compiler\": {\n        \"area\": 2667930.0,\n        \"max_frequency\": 17.232495661719216,\n        \"power\": 147.769\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n#include <string.h>\n\n// 定义常量\n#define VOCAB_SIZE 300       // 300000词表大小\n#define EMBEDDING_SIZE 128     // 分解嵌入维度\n#define HIDDEN_SIZE 128       //4096 隐藏层维度\n#define N_LAYERS 12            // Transformer 层数\n#define N_HEADS 64             // 多头注意力头数\n#define FFN_HIDDEN_SIZE 128  //16384 前馈网络隐藏层大小\n#define MAX_SEQ_LEN 64        //512 最大序列长度\n#define DROPOUT 0.1            // Dropout 概率\n#define DIM HIDDEN_SIZE        // 定义 dim 为 HIDDEN_SIZE\n#define HEAD_DIM (DIM / N_HEADS)  // 每个注意力头的维度\n\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\n\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\nvoid swap_nodes(void* a, void* b, size_t width) {\n    char* p1 = (char*)a;\n    char* p2 = (char*)b;\n    for (size_t i = 0; i < width; ++i) {\n        char temp = p1[i];\n        p1[i] = p2[i];\n        p2[i] = temp;\n    }\n}\n\n// 定义矩阵乘法\nvoid matmul(float* a, float* b, float* output, int m, int n, int k) {\n    for (int i = 0; i < m; i++) {\n        for (int j = 0; j < k; j++) {\n            output[i * k + j] = 0.0f;\n            for (int l = 0; l < n; l++) {\n                output[i * k + j] += a[i * n + l] * b[l * k + j];\n            }\n        }\n    }\n}\n\n// 定义 Softmax\nvoid softmax(float* x, int size) {\n    float max_val = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] > max_val) max_val = x[i];\n    }\n\n    float sum_exp = 0.0f;\n    for (int i = 0; i < size; i++) {\n        x[i] = expf(x[i] - max_val);\n        sum_exp += x[i];\n    }\n\n    for (int i = 0; i < size; i++) {\n        x[i] /= sum_exp;\n    }\n}\n\n// 定义 Layer Normalization\nvoid layer_norm(float* x, float* gamma, float* beta, int size) {\n    float mean = 0.0f, var = 0.0f;\n    for (int i = 0; i < size; i++) {\n        mean += x[i];\n    }\n    mean /= size;\n\n    for (int i = 0; i < size; i++) {\n        var += (x[i] - mean) * (x[i] - mean);\n    }\n    var /= size;\n\n    float std = sqrtf(var + 1e-5);\n    for (int i = 0; i < size; i++) {\n        x[i] = (x[i] - mean) / std * gamma[i] + beta[i];\n    }\n}\n\n// 定义多头注意力机制\nvoid multi_head_attention(float* query, float* key, float* value, float* output) {\n    float scores[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n    float attn_weights[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n\n    // 计算注意力分数\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        for (int j = 0; j < MAX_SEQ_LEN; j++) {\n            scores[i][j] = 0.0f;\n            for (int l = 0; l < HEAD_DIM; l++) {\n                scores[i][j] += query[i * HEAD_DIM + l] * key[j * HEAD_DIM + l];\n            }\n            scores[i][j] /= sqrtf(HEAD_DIM);\n        }\n    }\n\n    // Softmax\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        softmax(scores[i], MAX_SEQ_LEN);\n    }\n\n    // 加权求和\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        for (int j = 0; j < HEAD_DIM; j++) {\n            output[i * HEAD_DIM + j] = 0.0f;\n            for (int l = 0; l < MAX_SEQ_LEN; l++) {\n                output[i * HEAD_DIM + j] += scores[i][l] * value[l * HEAD_DIM + j];\n            }\n        }\n    }\n}\n\n// 定义前馈神经网络\nvoid feed_forward(float* x, float* w1, float* w2, float* output) {\n    float h[FFN_HIDDEN_SIZE];\n    matmul(x, w1, h, 1, DIM, FFN_HIDDEN_SIZE);\n    for (int i = 0; i < FFN_HIDDEN_SIZE; i++) {\n        h[i] = fmaxf(0.0f, h[i]);  // ReLU\n    }\n    matmul(h, w2, output, 1, FFN_HIDDEN_SIZE, DIM);\n}\n\n// 定义 Transformer Block\nvoid transformer_block(float* x, float* attn_weights, float* ffn_weights) {\n    float attn_output[DIM];\n    multi_head_attention(x, x, x, attn_output);\n\n    for (int i = 0; i < DIM; i++) {\n        x[i] += attn_output[i];  // 残差连接\n    }\n\n    float ffn_output[DIM];\n    feed_forward(x, ffn_weights, ffn_weights + DIM * FFN_HIDDEN_SIZE, ffn_output);\n\n    for (int i = 0; i < DIM; i++) {\n        x[i] += ffn_output[i];  // 残差连接\n    }\n}\n\n// 定义 ALBERT 模型\nvoid albert(float* input, float* embedding_weights, float* transformer_weights, float* output) {\n    float x[DIM]={0};\n    matmul(input, embedding_weights, x, 1, MAX_SEQ_LEN, DIM);\n\n    // 参数共享：所有层共享同一组权重\n    for (int i = 0; i < N_LAYERS; i++) {\n        transformer_block(x, transformer_weights, transformer_weights + DIM * DIM);\n    }\n\n    // 将最终输出复制到 output\n    memcpy(output, x, DIM * sizeof(float));\n}\n\n// 定义下一句预测（SOP）\nvoid sentence_order_prediction(float* cls_output, float* sop_weights, float* sop_bias, float* output) {\n    float logits[2];\n    matmul(cls_output, sop_weights, logits, 1, HIDDEN_SIZE, 2);\n    for (int i = 0; i < 2; i++) {\n        logits[i] += sop_bias[i];\n    }\n    softmax(logits, 2);\n    memcpy(output, logits, 2 * sizeof(float));\n}\n\n// 定义掩码语言模型（MLM）\nvoid masked_language_model(float* output, float* mlm_weights, float* mlm_bias, float* mlm_output) {\n    float logits[MAX_SEQ_LEN * VOCAB_SIZE];\n    matmul(output, mlm_weights, logits, MAX_SEQ_LEN, HIDDEN_SIZE, VOCAB_SIZE);\n    for (int i = 0; i < MAX_SEQ_LEN * VOCAB_SIZE; i++) {\n        logits[i] += mlm_bias[i % VOCAB_SIZE];\n    }\n    for (int i = 0; i < MAX_SEQ_LEN; i++) {\n        softmax(logits + i * VOCAB_SIZE, VOCAB_SIZE);\n    }\n    memcpy(mlm_output, logits, MAX_SEQ_LEN * VOCAB_SIZE * sizeof(float));\n}\n\nint main() {\n    // 初始化输入和权重\n    float input[MAX_SEQ_LEN] = {0};\n    float embedding_weights[VOCAB_SIZE * EMBEDDING_SIZE] = {0};\n    float transformer_weights[N_LAYERS * HIDDEN_SIZE * HIDDEN_SIZE * 2] = {0};\n    float sop_weights[HIDDEN_SIZE * 2] = {0};\n    float sop_bias[2] = {0};\n    float mlm_weights[HIDDEN_SIZE * VOCAB_SIZE] = {0};\n    float mlm_bias[VOCAB_SIZE] = {0};\n\n    // ALBERT 输出\n    float albert_output[HIDDEN_SIZE];\n    albert(input, embedding_weights, transformer_weights, albert_output);\n\n    \n    float sop_output[2];\n    sentence_order_prediction(albert_output, sop_weights, sop_bias, sop_output);\n\n    // 掩码语言模型\n    float mlm_output[MAX_SEQ_LEN * VOCAB_SIZE];\n    masked_language_model(albert_output, mlm_weights, mlm_bias, mlm_output);\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 295,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 28404,\n        \"dsp_count\": 0,\n        \"flip_flops\": 6423\n    },\n    \"silicon_compiler\": {\n        \"area\": 585140.0,\n        \"max_frequency\": 17.391122875239564,\n        \"power\": 55.2044\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n#include <string.h>\n#include <float.h>\n\n#define DIM 128 //128\n#define NUM_HEADS 8\n#define FF_DIM 128 //2048\n#define MAX_SEQ_LEN 64//512\n#define VOCAB_SIZE 128 //32128\n#define BEAM_SIZE 4\n#define NUM_LAYERS 12\n#define PREFIX_LEN 10\n#define EOS_TOKEN 1\n\ntypedef float Vector[DIM];\n\ntypedef struct {\n    float ffn_w1[NUM_LAYERS][DIM][FF_DIM];\n    float ffn_w2[NUM_LAYERS][FF_DIM][DIM];\n    Vector ln_gamma[NUM_LAYERS][2];\n    Vector ln_beta[NUM_LAYERS][2];\n    float vocab_weights[DIM][VOCAB_SIZE];\n    float prefix_weights[PREFIX_LEN][DIM];\n} T5Weights;\n\ntypedef struct {\n    int tokens[MAX_SEQ_LEN];\n    float score;\n    int length;\n} BeamNode;\n\nstatic unsigned long _rand_seed = 1;\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\nvoid swap_nodes(void* a, void* b, size_t width) {\n    char* p1 = (char*)a;\n    char* p2 = (char*)b;\n    for (size_t i = 0; i < width; ++i) {\n        char temp = p1[i];\n        p1[i] = p2[i];\n        p2[i] = temp;\n    }\n}\n\n// 手动实现的冒泡排序版qsort\nvoid qsort(void* base, size_t num, size_t width, \n                int (*cmp)(const void*, const void*)) \n{\n    char* array = (char*)base;\n    for (size_t i = 0; i < num - 1; ++i) {\n        for (size_t j = 0; j < num - 1 - i; ++j) {\n            // 计算相邻元素地址[2,4](@ref)\n            void* elem1 = array + j * width;\n            void* elem2 = array + (j + 1) * width;\n            \n            // 调用比较函数[1,3](@ref)\n            if (cmp(elem1, elem2) > 0) { \n                swap_nodes(elem1, elem2, width); // 执行交换\n            }\n        }\n    }\n}\n\n\n\n// 快速幂算法（仅支持正整数指数）\nfloat powf(float base, float exponent) {\n    // 验证指数必须为整数且为正数\n    if (exponent <= 0.0f) return 0.0f / 0.0f; // NaN\n    \n    // 检查是否为整数\n    unsigned int exp_int = (unsigned int)exponent;\n    if ((float)exp_int != exponent) return 0.0f / 0.0f;\n\n    // 处理特殊情况\n    if (base == 0.0f) return 0.0f;\n\n    // 负数底数处理\n    int sign = 1;\n    if (base < 0.0f) {\n        sign = (exp_int % 2 == 1) ? -1 : 1;\n        base = -base;\n    }\n\n    // 快速幂计算\n    float result = 1.0f;\n    while (exp_int > 0) {\n        if (exp_int & 1) result *= base;\n        base *= base;\n        exp_int >>= 1;\n    }\n    return sign * result;\n}\n\n// 指数函数近似（泰勒展开）\n\n\n// 平方根（牛顿迭代法）\nfloat sqrtf(float x) {\n    if (x <= 0) return 0.0f;\n    float guess = x / 2.0f;\n    for (int i = 0; i < 10; i++) {\n        guess = 0.5f * (guess + x / guess);\n    }\n    return guess;\n}\n\n// 添加矩阵乘法和softmax实现\nvoid matmul(float* A, float* B, float* C, int m, int k, int n) {\n    memset(C, 0, m*n*sizeof(float));\n    for (int i = 0; i < m; i++) {\n        for (int j = 0; j < n; j++) {\n            for (int l = 0; l < k; l++) {\n                C[i*n + j] += A[i*k + l] * B[l*n + j];\n            }\n        }\n    }\n}\n\nvoid softmax(float* arr, int size) {\n    float max_val = -FLT_MAX;\n    for (int i = 0; i < size; ++i)\n        if (arr[i] > max_val) max_val = arr[i];\n    \n    float sum = 0.0f;\n    for (int i = 0; i < size; ++i) {\n        arr[i] = expf(arr[i] - max_val);\n        sum += arr[i];\n    }\n    for (int i = 0; i < size; ++i)\n        arr[i] /= sum;\n}\n\nvoid t5_relative_bias(int seq_len, float bias[MAX_SEQ_LEN][MAX_SEQ_LEN]) {\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < seq_len; j++) {\n            int distance = i - j + (seq_len - 1);\n            bias[i][j] = (distance < 0) ? -FLT_MAX : 1.0f / sqrtf(distance + 1);\n        }\n    }\n}\n\nvoid layer_norm(float* x, Vector gamma, Vector beta) {\n    float mean = 0.0f, var = 0.0f;\n    for (int i = 0; i < DIM; i++) mean += x[i];\n    mean /= DIM;\n    for (int i = 0; i < DIM; i++) var += (x[i] - mean) * (x[i] - mean);\n    var /= DIM;\n    for (int i = 0; i < DIM; i++)\n        x[i] = (x[i] - mean)/sqrtf(var + 1e-6) * gamma[i] + beta[i];\n}\n\nvoid attention(float* x, float rel_bias[MAX_SEQ_LEN][MAX_SEQ_LEN], \n               float* output, int seq_len) {\n    float scores[MAX_SEQ_LEN][MAX_SEQ_LEN] = {0};\n    \n    // 计算注意力分数\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < seq_len; j++) {\n            for (int k = 0; k < DIM; k++) {\n                scores[i][j] += x[i*DIM + k] * x[j*DIM + k];\n            }\n            scores[i][j] = scores[i][j]/sqrtf(DIM) + rel_bias[i][j];\n        }\n        softmax(scores[i], seq_len);\n    }\n\n    // 计算加权和\n    for (int i = 0; i < seq_len; i++) {\n        for (int k = 0; k < DIM; k++) {\n            output[i*DIM + k] = 0;\n            for (int j = 0; j < seq_len; j++) {\n                output[i*DIM + k] += scores[i][j] * x[j*DIM + k];\n            }\n        }\n    }\n}\n\nvoid cross_attention(float* q, float* kv, float* output, \n                    int q_len, int kv_len) {\n    // 简化的cross-attention实现\n    for (int i = 0; i < q_len; i++) {\n        for (int j = 0; j < kv_len; j++) {\n            float score = 0;\n            for (int k = 0; k < DIM; k++) {\n                score += q[i*DIM + k] * kv[j*DIM + k];\n            }\n            score /= sqrtf(DIM);\n            \n            // 计算加权和\n            if (j == 0) {\n                for (int k = 0; k < DIM; k++) \n                    output[i*DIM + k] = score * kv[j*DIM + k];\n            } else {\n                for (int k = 0; k < DIM; k++) \n                    output[i*DIM + k] += score * kv[j*DIM + k];\n            }\n        }\n    }\n}\n\nvoid transformer_encoder_layer(float* x, int seq_len, int layer_idx,\n                              T5Weights* w) {\n    float rel_bias[MAX_SEQ_LEN][MAX_SEQ_LEN];\n    t5_relative_bias(seq_len, rel_bias);\n\n    // 自注意力\n    float attn_out[MAX_SEQ_LEN * DIM];\n    attention(x, rel_bias, attn_out, seq_len);\n    \n    // 残差连接+LayerNorm\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += attn_out[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][0], w->ln_beta[layer_idx][0]);\n    }\n\n    // 前馈网络\n    float ffn_out[MAX_SEQ_LEN * FF_DIM];\n    matmul(x, (float*)w->ffn_w1[layer_idx], ffn_out, seq_len, DIM, FF_DIM);\n    \n    // ReLU激活\n    for (int i = 0; i < seq_len*FF_DIM; i++)\n        ffn_out[i] = fmaxf(ffn_out[i], 0.0f);\n    \n    float ffn_final[MAX_SEQ_LEN * DIM];\n    matmul(ffn_out, (float*)w->ffn_w2[layer_idx], ffn_final, seq_len, FF_DIM, DIM);\n    \n    // 残差连接+LayerNorm\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += ffn_final[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][1], w->ln_beta[layer_idx][1]);\n    }\n}\n\nvoid transformer_decoder_layer(float* x, float* enc_out, int seq_len,\n                              int layer_idx, T5Weights* w) {\n    // 因果掩码\n    float causal_mask[MAX_SEQ_LEN][MAX_SEQ_LEN];\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < seq_len; j++) {\n            causal_mask[i][j] = (j > i) ? -FLT_MAX : 0.0f;\n        }\n    }\n\n    // 自注意力\n    float self_attn_out[MAX_SEQ_LEN * DIM];\n    attention(x, causal_mask, self_attn_out, seq_len);\n    \n    // 残差+LN\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += self_attn_out[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][0], w->ln_beta[layer_idx][0]);\n    }\n\n    // Cross-attention\n    float cross_attn_out[MAX_SEQ_LEN * DIM];\n    cross_attention(x, enc_out, cross_attn_out, seq_len, MAX_SEQ_LEN);\n    \n    // 残差+LN\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += cross_attn_out[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][1], w->ln_beta[layer_idx][1]);\n    }\n\n    // 前馈网络\n    float ffn_out[MAX_SEQ_LEN * FF_DIM];\n    matmul(x, (float*)w->ffn_w1[layer_idx], ffn_out, seq_len, DIM, FF_DIM);\n    \n    for (int i = 0; i < seq_len*FF_DIM; i++)\n        ffn_out[i] = fmaxf(ffn_out[i], 0.0f);\n    \n    float ffn_final[MAX_SEQ_LEN * DIM];\n    matmul(ffn_out, (float*)w->ffn_w2[layer_idx], ffn_final, seq_len, FF_DIM, DIM);\n    \n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < DIM; j++)\n            x[i*DIM + j] += ffn_final[i*DIM + j];\n        layer_norm(x + i*DIM, w->ln_gamma[layer_idx][1], w->ln_beta[layer_idx][1]);\n    }\n}\n\nvoid encoder(float* x, int seq_len, T5Weights* w) {\n    for (int layer = 0; layer < NUM_LAYERS; layer++)\n        transformer_encoder_layer(x, seq_len, layer, w);\n}\n\nvoid decoder(float* x, float* enc_out, int seq_len, T5Weights* w) {\n    for (int layer = 0; layer < NUM_LAYERS; layer++)\n        transformer_decoder_layer(x, enc_out, seq_len, layer, w);\n}\n\nvoid apply_prefix(float* x, int seq_len, T5Weights* w) {\n    for (int i = 0; i < PREFIX_LEN; i++)\n        memcpy(x + (seq_len + i)*DIM, w->prefix_weights[i], DIM*sizeof(float));\n}\n\nint compare_nodes(const void* a, const void* b) {\n    return ((BeamNode*)b)->score > ((BeamNode*)a)->score ? 1 : -1;\n}\n\nvoid beam_search(T5Weights* w, float* enc_out) {\n    BeamNode beams[BEAM_SIZE] = {0};\n    beams[0].score = 0.0f;\n    beams[0].tokens[0] = 0;  // Start token\n    beams[0].length = 1;\n    int current_beam_size = 1;\n\n    for (int step = 0; step < MAX_SEQ_LEN; step++) {\n        BeamNode new_beams[BEAM_SIZE * VOCAB_SIZE] = {0};\n        int new_count = 0;\n\n        for (int i = 0; i < current_beam_size; i++) {\n            float input[MAX_SEQ_LEN * DIM] = {0};\n            memcpy(input, enc_out, MAX_SEQ_LEN*DIM*sizeof(float));\n\n            // 生成decoder输入\n            int curr_len = beams[i].length;\n            for (int j = 0; j < curr_len; j++)\n                input[j*DIM] = (float)beams[i].tokens[j];\n\n            decoder(input, enc_out, curr_len, w);\n\n            // 获取logits\n            float logits[VOCAB_SIZE] = {0};\n            matmul(input + (curr_len-1)*DIM, (float*)w->vocab_weights, \n                  logits, 1, DIM, VOCAB_SIZE);\n            \n            softmax(logits, VOCAB_SIZE);\n\n            // 收集候选\n            for (int j = 0; j < VOCAB_SIZE; j++) {\n                if (new_count >= BEAM_SIZE * VOCAB_SIZE) break;\n                \n                new_beams[new_count] = beams[i];\n                new_beams[new_count].tokens[curr_len] = j;\n                new_beams[new_count].score += logf(logits[j] + 1e-12);\n                new_beams[new_count].length = curr_len + 1;\n                new_count++;\n            }\n        }\n\n        qsort(new_beams, new_count, sizeof(BeamNode), compare_nodes);\n\n        current_beam_size = 0;\n        for (int i = 0; i < BEAM_SIZE && i < new_count; i++) {\n            \n            beams[current_beam_size++] = new_beams[i];\n        }\n    }\n}\n\nvoid init_weights(T5Weights* w) {\n    for (int l = 0; l < NUM_LAYERS; l++) {\n        for (int i = 0; i < DIM; i++) {\n            for (int j = 0; j < FF_DIM; j++) {\n                w->ffn_w1[l][i][j] = (rand()/(float)RAND_MAX - 0.5f) * 0.1f;\n                w->ffn_w2[l][j][i] = (rand()/(float)RAND_MAX - 0.5f) * 0.1f;\n            }\n            w->ln_gamma[l][0][i] = 1.0f;\n            w->ln_beta[l][0][i] = 0.0f;\n            w->ln_gamma[l][1][i] = 1.0f;\n            w->ln_beta[l][1][i] = 0.0f;\n        }\n    }\n    for (int i = 0; i < DIM; i++) {\n        for (int j = 0; j < VOCAB_SIZE; j++) {\n            w->vocab_weights[i][j] = (rand()/(float)RAND_MAX - 0.5f) * 0.1f;\n        }\n    }\n    for (int i = 0; i < PREFIX_LEN; i++) {\n        for (int j = 0; j < DIM; j++) {\n            w->prefix_weights[i][j] = (rand()/(float)RAND_MAX - 0.5f) * 0.1f;\n        }\n    }\n}\n\nint main() {\n    T5Weights weights;\n    init_weights(&weights);\n\n    float enc_input[MAX_SEQ_LEN * DIM] = {0};\n    int seq_len = 5;\n    apply_prefix(enc_input, seq_len, &weights);\n    \n    encoder(enc_input, seq_len + PREFIX_LEN, &weights);\n    \n    beam_search(&weights, enc_input);\n    \n    \n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 1381,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 42291,\n        \"dsp_count\": 12,\n        \"flip_flops\": 19635\n    },\n    \"silicon_compiler\": {\n        \"area\": 1573790.0,\n        \"max_frequency\": 20.976145926851984,\n        \"power\": 511.047\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n// #include <math.h>\n#include <string.h>\ndouble expf(double x)\n{\n    double result = 1.0; // 初始化\n    double term = 1.0;   // 泰勒展开的第一个项\n    for (int i = 1; i <= 10; ++i)\n    {\n        term *= x / i;  // 计算下一项\n        result += term; // 累加到结果中\n    }\n    return result;\n}\nfloat sqrtf(float x) {\n    if (x <= 0) return 0.0f;  // 防止负数\n    float guess = x / 2.0f;  // 初始猜测值\n    float result;\n    \n    // 牛顿迭代法\n    for (int i = 0; i < 10; i++) {\n        result = 0.5f * (guess + x / guess);\n        guess = result;\n    }\n\n    return result;\n}\n// 定义常量\n#define DIM  64           // 模型维度\n#define N_HEADS 8         // 多头注意力头数\n#define HEAD_DIM (DIM / N_HEADS) // 每个头的维度\n#define MAX_SEQ_LEN 64   // 最大序列长度\n#define EPS 1e-5          // 归一化的 epsilon\n#define N_LAYERS 6        // Transformer 层数\n#define HIDDEN_DIM (DIM * 4) // 前馈网络的隐藏层维度\n\n// 定义数组大小\n#define TOKENS_SIZE MAX_SEQ_LEN\n#define TOK_EMBEDDINGS_SIZE (DIM * MAX_SEQ_LEN)\n#define OUTPUT_WEIGHTS_SIZE (DIM * MAX_SEQ_LEN)\n#define ATTN_NORM_WEIGHTS_SIZE DIM\n#define FFN_NORM_WEIGHTS_SIZE DIM\n#define WQ_SIZE (DIM * DIM)\n#define WK_SIZE (DIM * DIM)\n#define WV_SIZE (DIM * DIM)\n#define WO_SIZE (DIM * DIM)\n#define W1_SIZE (DIM * HIDDEN_DIM)\n#define W2_SIZE (HIDDEN_DIM * DIM)\n#define W3_SIZE (DIM * HIDDEN_DIM)\n\n// RMSNorm 层\nvoid rms_norm(float* x, float* weight, float* output) {\n    float mean_square = 0.0f;\n    for (int i = 0; i < DIM; i++) {\n        mean_square += x[i] * x[i];\n    }\n    mean_square /= DIM;\n\n    float rsqrt_val = 1.0f / sqrtf(mean_square + EPS);\n    for (int i = 0; i < DIM; i++) {\n        output[i] = x[i] * rsqrt_val * weight[i];\n    }\n}\n\n// 矩阵乘法\nvoid matmul(float* a, float* b, float* output, int m, int n, int k) {\n    for (int i = 0; i < m; i++) {\n        for (int j = 0; j < k; j++) {\n            output[i * k + j] = 0.0f;\n            for (int l = 0; l < n; l++) {\n                output[i * k + j] += a[i * n + l] * b[l * k + j];\n            }\n        }\n    }\n}\n\n// 注意力机制\nvoid attention(float* q, float* k, float* v, float* output, int seq_len) {\n    float scores[MAX_SEQ_LEN * MAX_SEQ_LEN];\n    float attn_weights[MAX_SEQ_LEN * MAX_SEQ_LEN];\n\n    // 计算注意力分数\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < seq_len; j++) {\n            scores[i * seq_len + j] = 0.0f;\n            for (int l = 0; l < HEAD_DIM; l++) {\n                scores[i * seq_len + j] += q[i * HEAD_DIM + l] * k[j * HEAD_DIM + l];\n            }\n            scores[i * seq_len + j] /= sqrtf(HEAD_DIM);\n        }\n    }\n\n    // Softmax\n    for (int i = 0; i < seq_len; i++) {\n        float max_score = scores[i * seq_len];\n        for (int j = 1; j < seq_len; j++) {\n            if (scores[i * seq_len + j] > max_score) {\n                max_score = scores[i * seq_len + j];\n            }\n        }\n\n        float sum_exp = 0.0f;\n        for (int j = 0; j < seq_len; j++) {\n            attn_weights[i * seq_len + j] = expf(scores[i * seq_len + j] - max_score);\n            sum_exp += attn_weights[i * seq_len + j];\n        }\n\n        for (int j = 0; j < seq_len; j++) {\n            attn_weights[i * seq_len + j] /= sum_exp;\n        }\n    }\n\n    // 加权求和\n    for (int i = 0; i < seq_len; i++) {\n        for (int j = 0; j < HEAD_DIM; j++) {\n            output[i * HEAD_DIM + j] = 0.0f;\n            for (int l = 0; l < seq_len; l++) {\n                output[i * HEAD_DIM + j] += attn_weights[i * seq_len + l] * v[l * HEAD_DIM + j];\n            }\n        }\n    }\n}\n\n// 前馈神经网络\nvoid feed_forward(float* x, float* w1, float* w2, float* w3, float* output) {\n    float h1[HIDDEN_DIM];\n    float h2[HIDDEN_DIM];\n\n    // w1(x) 和 SiLU 激活\n    matmul(x, w1, h1, 1, DIM, HIDDEN_DIM);\n    for (int i = 0; i < HIDDEN_DIM; i++) {\n        h1[i] = h1[i] / (1 + expf(-h1[i])); // SiLU\n    }\n\n    // w3(x)\n    matmul(x, w3, h2, 1, DIM, HIDDEN_DIM);\n\n    // w2(SiLU(w1(x)) * w3(x))\n    for (int i = 0; i < HIDDEN_DIM; i++) {\n        h2[i] *= h1[i];\n    }\n    matmul(h2, w2, output, 1, HIDDEN_DIM, DIM);\n}\n\n// Transformer 层\nvoid transformer_block(float* x, float* attn_norm_weight, float* ffn_norm_weight,\n                       float* wq, float* wk, float* wv, float* wo,\n                       float* w1, float* w2, float* w3, float* output, int seq_len) {\n    float norm_output[DIM];\n    float attn_output[DIM];\n    float ffn_output[DIM];\n\n    // 注意力层\n    rms_norm(x, attn_norm_weight, norm_output);\n    attention(norm_output, norm_output, norm_output, attn_output, seq_len);\n    for (int i = 0; i < DIM; i++) {\n        attn_output[i] += x[i];\n    }\n\n    // 前馈层\n    rms_norm(attn_output, ffn_norm_weight, norm_output);\n    feed_forward(norm_output, w1, w2, w3, ffn_output);\n    for (int i = 0; i < DIM; i++) {\n        output[i] = attn_output[i] + ffn_output[i];\n    }\n}\n\n// Transformer 模型\nvoid transformer(float* tokens, float* tok_embeddings, float* output_weights,\n                 float* attn_norm_weights, float* ffn_norm_weights,\n                 float* wq, float* wk, float* wv, float* wo,\n                 float* w1, float* w2, float* w3, int seq_len) {\n    float h[DIM];\n    matmul(tokens, tok_embeddings, h, 1, seq_len, DIM);\n\n    for (int i = 0; i < N_LAYERS; i++) {\n        transformer_block(h, attn_norm_weights, ffn_norm_weights,\n                         wq, wk, wv, wo, w1, w2, w3, h, seq_len);\n    }\n\n    rms_norm(h, attn_norm_weights, h);\n    matmul(h, output_weights, tokens, 1, DIM, seq_len);\n}\n\nint main() {\n    int seq_len = 10;\n\n    // 定义静态数组\n    float tokens[TOKENS_SIZE];\n    float tok_embeddings[TOK_EMBEDDINGS_SIZE];\n    float output_weights[OUTPUT_WEIGHTS_SIZE];\n    float attn_norm_weights[ATTN_NORM_WEIGHTS_SIZE];\n    float ffn_norm_weights[FFN_NORM_WEIGHTS_SIZE];\n    float wq[WQ_SIZE];\n    float wk[WK_SIZE];\n    float wv[WV_SIZE];\n    float wo[WO_SIZE];\n    float w1[W1_SIZE];\n    float w2[W2_SIZE];\n    float w3[W3_SIZE];\n\n    // 初始化数据\n    for (int i = 0; i < seq_len; i++) {\n        tokens[i] = 1.0f; // 假设输入全为 1\n    }\n    for (int i = 0; i < DIM * seq_len; i++) {\n        tok_embeddings[i] = 0.01f * (i % DIM); // 假设嵌入权重\n        output_weights[i] = 0.01f * (i % DIM); // 假设输出权重\n    }\n    for (int i = 0; i < DIM; i++) {\n        attn_norm_weights[i] = 1.0f; // 假设归一化权重\n        ffn_norm_weights[i] = 1.0f;\n    }\n    for (int i = 0; i < DIM * DIM; i++) {\n        wq[i] = 0.01f * (i % DIM); // 假设注意力权重\n        wk[i] = 0.01f * (i % DIM);\n        wv[i] = 0.01f * (i % DIM);\n        wo[i] = 0.01f * (i % DIM);\n    }\n    for (int i = 0; i < DIM * HIDDEN_DIM; i++) {\n        w1[i] = 0.01f * (i % DIM); // 假设前馈权重\n        w3[i] = 0.01f * (i % DIM);\n    }\n    for (int i = 0; i < HIDDEN_DIM * DIM; i++) {\n        w2[i] = 0.01f * (i % DIM);\n    }\n\n    // 调用 Transformer\n    transformer(tokens, tok_embeddings, output_weights,\n               attn_norm_weights, ffn_norm_weights,\n               wq, wk, wv, wo, w1, w2, w3, seq_len);\n\n    // 打印输出\n  \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 917,\n        \"min_slack\": 3.4927616354707425e-13,\n        \"max_frequency\": 100.0000000000035,\n        \"estimated_resources_area\": 30438,\n        \"dsp_count\": 0,\n        \"flip_flops\": 12966\n    },\n    \"silicon_compiler\": {\n        \"area\": 1326080.0,\n        \"max_frequency\": 18.957705359343304,\n        \"power\": 145.74300000000002\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <string.h>\n\n// ================== 自定义数学函数实现 ==================\n// 伪随机数生成器（线性同余算法）\nstatic unsigned long _rand_seed = 1;\n\nint my_rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n\n// 双曲正切近似计算\nfloat my_tanhf(float x) {\n    // 限制输入范围防止溢出\n    if (x > 4.97f) return 1.0f;\n    if (x < -4.97f) return -1.0f;\n    \n    float x2 = x * x;\n    float x3 = x * x2;\n    float x5 = x3 * x2;\n    float x7 = x5 * x2;\n    return x - x3/3.0f + 2.0f*x5/15.0f - 17.0f*x7/315.0f;\n}\n\n// 快速幂算法（仅支持正整数指数）\nfloat powf(float base, float exponent) {\n    // 验证指数必须为整数且为正数\n    if (exponent <= 0.0f) return 0.0f / 0.0f; // NaN\n    \n    // 检查是否为整数\n    unsigned int exp_int = (unsigned int)exponent;\n    if ((float)exp_int != exponent) return 0.0f / 0.0f;\n\n    // 处理特殊情况\n    if (base == 0.0f) return 0.0f;\n\n    // 负数底数处理\n    int sign = 1;\n    if (base < 0.0f) {\n        sign = (exp_int % 2 == 1) ? -1 : 1;\n        base = -base;\n    }\n\n    // 快速幂计算\n    float result = 1.0f;\n    while (exp_int > 0) {\n        if (exp_int & 1) result *= base;\n        base *= base;\n        exp_int >>= 1;\n    }\n    return sign * result;\n}\n\n// 指数函数近似（泰勒展开）\ndouble expf(double x) {\n    double result = 1.0;\n    double term = 1.0;\n    for (int i = 1; i <= 10; ++i) {\n        term *= x / i;\n        result += term;\n    }\n    return result;\n}\n\n\n// 平方根（牛顿迭代法）\nfloat sqrtf(float x) {\n    if (x <= 0) return 0.0f;\n    float guess = x / 2.0f;\n    for (int i = 0; i < 10; i++) {\n        guess = 0.5f * (guess + x / guess);\n    }\n    return guess;\n}\n\n// ================== 模型配置参数 ==================\n#define HIDDEN_SIZE 64\n#define NUM_LAYERS 10\n#define NUM_HEADS 8\n#define SEQ_LEN 64\n#define VOCAB_SIZE 128\n#define INTERMEDIATE_SIZE 256\n#define BATCH_SIZE 2\n#define EPS 1e-5\n\n// ================== 模型数据结构 ==================\nstatic float embeddings[VOCAB_SIZE][HIDDEN_SIZE];\nstatic float position_emb[SEQ_LEN][HIDDEN_SIZE];\nstatic float layer_norm_buffer[BATCH_SIZE][HIDDEN_SIZE];\nstatic float attention_output[BATCH_SIZE][SEQ_LEN][HIDDEN_SIZE];\nstatic float ff_intermediate[BATCH_SIZE][INTERMEDIATE_SIZE];\nstatic float layer_output[NUM_LAYERS][BATCH_SIZE][HIDDEN_SIZE];\n\n// ================== 模型核心实现 ==================\nvoid init_parameters() {\n    for (int i = 0; i < VOCAB_SIZE; i++) {\n        for (int j = 0; j < HIDDEN_SIZE; j++) {\n            embeddings[i][j] = (float)my_rand()/32767.0f * 0.02 - 0.01;\n        }\n    }\n    \n    for (int i = 0; i < SEQ_LEN; i++) {\n        for (int j = 0; j < HIDDEN_SIZE; j++) {\n            position_emb[i][j] = (float)my_rand()/32767.0f * 0.02 - 0.01;\n        }\n    }\n}\n\nvoid absolute_position_embedding(int batch_size) {\n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < SEQ_LEN; i++) {\n            for (int j = 0; j < HIDDEN_SIZE; j++) {\n                layer_output[0][b][i*HIDDEN_SIZE+j] += position_emb[i][j];\n            }\n        }\n    }\n}\n\nvoid layer_norm(int batch_size, float input[BATCH_SIZE][HIDDEN_SIZE]) {\n    for (int b = 0; b < batch_size; b++) {\n        float mean = 0.0, var = 0.0;\n        for (int i = 0; i < HIDDEN_SIZE; i++) mean += input[b][i];\n        mean /= HIDDEN_SIZE;\n        \n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            float diff = input[b][i] - mean;\n            var += diff * diff;\n        }\n        var /= HIDDEN_SIZE;\n        \n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            layer_norm_buffer[b][i] = (input[b][i] - mean) / sqrtf(var + EPS);\n        }\n    }\n}\n\nvoid multi_head_attention(int layer_id, int batch_size) {\n    const int head_dim = HIDDEN_SIZE / NUM_HEADS;\n    static float q_proj[NUM_LAYERS][HIDDEN_SIZE][HIDDEN_SIZE];\n    static float k_proj[NUM_LAYERS][HIDDEN_SIZE][HIDDEN_SIZE];\n    static float v_proj[NUM_LAYERS][HIDDEN_SIZE][HIDDEN_SIZE];\n    \n    // 初始化投影矩阵\n    for (int i = 0; i < HIDDEN_SIZE; i++) {\n        for (int j = 0; j < HIDDEN_SIZE; j++) {\n            q_proj[layer_id][i][j] = 0.01f * (i + j);\n            k_proj[layer_id][i][j] = 0.01f * (i - j);\n            v_proj[layer_id][i][j] = 0.01f * (i * j);\n        }\n    }\n    \n    // 计算注意力\n    for (int b = 0; b < batch_size; b++) {\n        for (int h = 0; h < NUM_HEADS; h++) {\n            for (int i = 0; i < SEQ_LEN; i++) {\n                for (int j = 0; j < SEQ_LEN; j++) {\n                    float score = 0.0;\n                    for (int k = 0; k < head_dim; k++) {\n                        float q = 0.0, k_val = 0.0;\n                        for (int m = 0; m < HIDDEN_SIZE; m++) {\n                            q += layer_output[layer_id][b][m] * q_proj[layer_id][m][h*head_dim + k];\n                            k_val += layer_output[layer_id][b][m] * k_proj[layer_id][m][h*head_dim + k];\n                        }\n                        score += q * k_val;\n                    }\n                    score /= sqrtf(head_dim);\n                    float attention = expf(score) / (1.0f + expf(score));\n                    \n                    for (int k = 0; k < head_dim; k++) {\n                        float v = 0.0;\n                        for (int m = 0; m < HIDDEN_SIZE; m++) {\n                            v += layer_output[layer_id][b][m] * v_proj[layer_id][m][h*head_dim + k];\n                        }\n                        attention_output[b][i][h*head_dim + k] += attention * v;\n                    }\n                }\n            }\n        }\n    }\n}\n\nvoid feed_forward(int layer_id, int batch_size) {\n    static float w1[NUM_LAYERS][HIDDEN_SIZE][INTERMEDIATE_SIZE];\n    static float w2[NUM_LAYERS][INTERMEDIATE_SIZE][HIDDEN_SIZE];\n    \n    // 初始化权重\n    for (int i = 0; i < HIDDEN_SIZE; i++) {\n        for (int j = 0; j < INTERMEDIATE_SIZE; j++) {\n            w1[layer_id][i][j] = 0.01f * (i + j);\n        }\n    }\n    \n    for (int i = 0; i < INTERMEDIATE_SIZE; i++) {\n        for (int j = 0; j < HIDDEN_SIZE; j++) {\n            w2[layer_id][i][j] = 0.01f * (i - j);\n        }\n    }\n    \n    // 中间层计算\n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < INTERMEDIATE_SIZE; i++) {\n            ff_intermediate[b][i] = 0;\n            for (int j = 0; j < HIDDEN_SIZE; j++) {\n                ff_intermediate[b][i] += layer_norm_buffer[b][j] * w1[layer_id][j][i];\n            }\n            // GELU激活函数\n            ff_intermediate[b][i] = 0.5f * ff_intermediate[b][i] * \n                                   (1.0f + my_tanhf(0.7978845608f * \n                                   (ff_intermediate[b][i] + 0.044715f * powf(ff_intermediate[b][i], 3))));\n        }\n    }\n    \n    // 输出层\n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            layer_output[layer_id+1][b][i] = 0;\n            for (int j = 0; j < INTERMEDIATE_SIZE; j++) {\n                layer_output[layer_id+1][b][i] += ff_intermediate[b][j] * w2[layer_id][j][i];\n            }\n        }\n    }\n}\n\nvoid transformer_layer(int layer_id, int batch_size) {\n    multi_head_attention(layer_id, batch_size);\n    \n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            attention_output[b][0][i] += layer_output[layer_id][b][i];\n        }\n    }\n    \n    layer_norm(batch_size, attention_output[0]);\n    feed_forward(layer_id, batch_size);\n    \n    for (int b = 0; b < batch_size; b++) {\n        for (int i = 0; i < HIDDEN_SIZE; i++) {\n            layer_output[layer_id+1][b][i] += layer_norm_buffer[b][i];\n        }\n    }\n}\n\n// ================== 主程序 ==================\nint main() {\n    // 初始化模型\n    init_parameters();\n    \n    // 准备输入数据\n    int input_ids[BATCH_SIZE][SEQ_LEN];\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int i = 0; i < SEQ_LEN; i++) {\n            input_ids[b][i] = my_rand() % VOCAB_SIZE;\n        }\n    }\n    \n    // 词嵌入查找\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int i = 0; i < SEQ_LEN; i++) {\n            memcpy(layer_output[0][b] + i*HIDDEN_SIZE, \n                  embeddings[input_ids[b][i]], \n                  HIDDEN_SIZE * sizeof(float));\n        }\n    }\n    \n    // 前向传播\n    for (int l = 0; l < NUM_LAYERS; l++) {\n        transformer_layer(l, BATCH_SIZE);\n    }\n    \n    // 输出结果\n   \n    \n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 884,\n        \"min_slack\": 1.1102230246251565e-15,\n        \"max_frequency\": 100.00000000000001,\n        \"estimated_resources_area\": 34913,\n        \"dsp_count\": 15,\n        \"flip_flops\": 15671\n    },\n    \"silicon_compiler\": {\n        \"area\": 1425740.0,\n        \"max_frequency\": 18.494988782789303,\n        \"power\": 237.642\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 128\n#define OUT_CHANNELS 128\n#define IN_HEIGHT 128\n#define IN_WIDTH 128\n#define POOL1_SIZE 32\n#define POOL2_SIZE 16\n#define POOL3_SIZE 8\n#define EPSILON 1e-5\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float pool1_output[BATCH_SIZE][OUT_CHANNELS / 4][POOL1_SIZE][POOL1_SIZE];\nstatic float pool2_output[BATCH_SIZE][OUT_CHANNELS / 4][POOL2_SIZE][POOL2_SIZE];\nstatic float pool3_output[BATCH_SIZE][OUT_CHANNELS / 4][POOL3_SIZE][POOL3_SIZE];\nstatic float pool4_output[BATCH_SIZE][OUT_CHANNELS / 4][IN_HEIGHT][IN_WIDTH];\nstatic float spp_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT / 2][IN_WIDTH / 2];\nstatic float conv1_output[BATCH_SIZE][OUT_CHANNELS / 2][IN_HEIGHT / 2][IN_WIDTH / 2];\nstatic float output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT / 2][IN_WIDTH / 2];\n\nstatic unsigned long _rand_seed = 1;\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\n\n// 4x4池化 + 1x1卷积 + 上采样\nvoid pool1() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 4; oc++) {\n            for (int h = 0; h < POOL1_SIZE; h++) {\n                for (int w = 0; w < POOL1_SIZE; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h * 4][w * 4] * 0.01;  // 假设权重为0.01\n                    }\n                    pool1_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 8x8池化 + 1x1卷积 + 上采样\nvoid pool2() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 4; oc++) {\n            for (int h = 0; h < POOL2_SIZE; h++) {\n                for (int w = 0; w < POOL2_SIZE; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h * 8][w * 8] * 0.01;  // 假设权重为0.01\n                    }\n                    pool2_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 16x16池化 + 1x1卷积 + 上采样\nvoid pool3() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 4; oc++) {\n            for (int h = 0; h < POOL3_SIZE; h++) {\n                for (int w = 0; w < POOL3_SIZE; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h * 16][w * 16] * 0.01;  // 假设权重为0.01\n                    }\n                    pool3_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 原始输入 + 1x1卷积\nvoid pool4() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 4; oc++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                        sum += input[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    pool4_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 多尺度拼接\nvoid spp_concat() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT / 2; h++) {\n                for (int w = 0; w < IN_WIDTH / 2; w++) {\n                    if (c < OUT_CHANNELS / 4) {\n                        spp_output[b][c][h][w] = pool1_output[b][c][h][w];\n                    } else if (c < OUT_CHANNELS / 2) {\n                        spp_output[b][c][h][w] = pool2_output[b][c - OUT_CHANNELS / 4][h][w];\n                    } else if (c < 3 * OUT_CHANNELS / 4) {\n                        spp_output[b][c][h][w] = pool3_output[b][c - OUT_CHANNELS / 2][h][w];\n                    } else {\n                        spp_output[b][c][h][w] = pool4_output[b][c - 3 * OUT_CHANNELS / 4][h][w];\n                    }\n                }\n            }\n        }\n    }\n}\n\n// 1x1卷积 (通道融合)\nvoid conv1() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS / 2; oc++) {\n            for (int h = 0; h < IN_HEIGHT / 2; h++) {\n                for (int w = 0; w < IN_WIDTH / 2; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < OUT_CHANNELS; ic++) {\n                        sum += spp_output[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    conv1_output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 1x1卷积 (通道融合)\nvoid conv2() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < OUT_CHANNELS; oc++) {\n            for (int h = 0; h < IN_HEIGHT / 2; h++) {\n                for (int w = 0; w < IN_WIDTH / 2; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < OUT_CHANNELS / 2; ic++) {\n                        sum += conv1_output[b][ic][h][w] * 0.01;  // 假设权重为0.01\n                    }\n                    output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 打印输出\n\n\n// 主函数\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    pool1();      // 4x4池化\n    pool2();      // 8x8池化\n    pool3();      // 16x16池化\n    pool4();      // 原始输入\n    spp_concat(); // 多尺度拼接\n    conv1();      // 1x1卷积 (通道融合)\n    conv2();      // 1x1卷积 (通道融合)\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 242,\n        \"min_slack\": 0.04769998999996372,\n        \"max_frequency\": 100.47928609418965,\n        \"estimated_resources_area\": 15277,\n        \"dsp_count\": 5,\n        \"flip_flops\": 4505\n    },\n    \"silicon_compiler\": {\n        \"area\": 285727.0,\n        \"max_frequency\": 19.226147560682527,\n        \"power\": 44.8615\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 256\n#define OUT_CHANNELS 512\n#define IN_HEIGHT 64\n#define IN_WIDTH 64\n#define RATIO 16\n#define KERNEL_SIZE 7\n#define GROUPS 8\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float avg_pool_output[BATCH_SIZE][IN_CHANNELS][1][1];\nstatic float fc1_output[BATCH_SIZE][IN_CHANNELS / RATIO];\nstatic float fc2_output[BATCH_SIZE][IN_CHANNELS];\nstatic float channel_att_output[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float avg_spatial[BATCH_SIZE][1][IN_HEIGHT][IN_WIDTH];\nstatic float max_spatial[BATCH_SIZE][1][IN_HEIGHT][IN_WIDTH];\nstatic float spatial_att_output[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float cbam_output[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float fpn_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT * 2][IN_WIDTH * 2];\n\n\nstatic unsigned long _rand_seed = 1;\n\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\n\n// 全局平均池化\nvoid global_avg_pool() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            float sum = 0.0;\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    sum += input[b][c][h][w];\n                }\n            }\n            avg_pool_output[b][c][0][0] = sum / (IN_HEIGHT * IN_WIDTH);\n        }\n    }\n}\n\n// 全连接层 (通道压缩)\nvoid fc1() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < IN_CHANNELS / RATIO; oc++) {\n            float sum = 0.0;\n            for (int ic = 0; ic < IN_CHANNELS; ic++) {\n                sum += avg_pool_output[b][ic][0][0] * 0.01;  // 假设权重为0.01\n            }\n            fc1_output[b][oc] = fmaxf(0.0, sum);  // ReLU\n        }\n    }\n}\n\n// 全连接层 (通道恢复)\nvoid fc2() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < IN_CHANNELS; oc++) {\n            float sum = 0.0;\n            for (int ic = 0; ic < IN_CHANNELS / RATIO; ic++) {\n                sum += fc1_output[b][ic] * 0.01;  // 假设权重为0.01\n            }\n            fc2_output[b][oc] = 1.0 / (1.0 + expf(-sum));  // Sigmoid\n        }\n    }\n}\n\n// 通道注意力\nvoid channel_attention() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    channel_att_output[b][c][h][w] = input[b][c][h][w] * fc2_output[b][c];\n                }\n            }\n        }\n    }\n}\n\n// 空间平均池化\nvoid spatial_avg_pool() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int h = 0; h < IN_HEIGHT; h++) {\n            for (int w = 0; w < IN_WIDTH; w++) {\n                float sum = 0.0;\n                for (int c = 0; c < IN_CHANNELS; c++) {\n                    sum += channel_att_output[b][c][h][w];\n                }\n                avg_spatial[b][0][h][w] = sum / IN_CHANNELS;\n            }\n        }\n    }\n}\n\n// 空间最大池化\nvoid spatial_max_pool() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int h = 0; h < IN_HEIGHT; h++) {\n            for (int w = 0; w < IN_WIDTH; w++) {\n                float max_val = -INFINITY;\n                for (int c = 0; c < IN_CHANNELS; c++) {\n                    max_val = fmaxf(max_val, channel_att_output[b][c][h][w]);\n                }\n                max_spatial[b][0][h][w] = max_val;\n            }\n        }\n    }\n}\n\n// 空间注意力 (7x7卷积)\nvoid spatial_attention() {\n    int padding = KERNEL_SIZE / 2;\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int h = 0; h < IN_HEIGHT; h++) {\n            for (int w = 0; w < IN_WIDTH; w++) {\n                float sum = 0.0;\n                for (int kh = 0; kh < KERNEL_SIZE; kh++) {\n                    for (int kw = 0; kw < KERNEL_SIZE; kw++) {\n                        int ih = h + kh - padding;\n                        int iw = w + kw - padding;\n                        if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                            sum += (avg_spatial[b][0][ih][iw] + max_spatial[b][0][ih][iw]) * 0.01;  // 假设权重为0.01\n                        }\n                    }\n                }\n                spatial_att_output[b][0][h][w] = 1.0 / (1.0 + expf(-sum));  // Sigmoid\n            }\n        }\n    }\n}\n\n// CBAM模块\nvoid cbam() {\n    channel_attention();\n    spatial_attention();\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    cbam_output[b][c][h][w] = channel_att_output[b][c][h][w] * spatial_att_output[b][0][h][w];\n                }\n            }\n        }\n    }\n}\n\n// FPN横向连接块 (分组卷积)\nvoid fpn_block(int in_channels, int out_channels, float (*input)[IN_CHANNELS][IN_HEIGHT][IN_WIDTH], float (*output)[OUT_CHANNELS][IN_HEIGHT * 2][IN_WIDTH * 2]) {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int oc = 0; oc < out_channels; oc++) {\n            for (int h = 0; h < IN_HEIGHT * 2; h++) {\n                for (int w = 0; w < IN_WIDTH * 2; w++) {\n                    float sum = 0.0;\n                    for (int ic = 0; ic < in_channels / GROUPS; ic++) {\n                        for (int kh = 0; kh < 3; kh++) {\n                            for (int kw = 0; kw < 3; kw++) {\n                                int ih = h / 2 + kh - 1;\n                                int iw = w / 2 + kw - 1;\n                                if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                    sum += input[b][ic][ih][iw] * 0.01;  // 假设权重为0.01\n                                }\n                            }\n                        }\n                    }\n                    output[b][oc][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 轻量化FPN\nvoid light_fpn() {\n    // CBAM处理\n    cbam();\n    \n    // FPN自上而下路径\n    fpn_block(512, 256, cbam_output, fpn_output);  // 假设高层特征输入为512通道\n}\n\n// 打印输出\n\n\n// 主函数\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 前向传播\n    light_fpn();\n\n    // 打印结果\n   // print_output();\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 250,\n        \"min_slack\": 8.215650382226158e-15,\n        \"max_frequency\": 100.00000000000009,\n        \"estimated_resources_area\": 22678,\n        \"dsp_count\": 3,\n        \"flip_flops\": 3774\n    },\n    \"silicon_compiler\": {\n        \"area\": 403389.0,\n        \"max_frequency\": 18.986428500907554,\n        \"power\": 116.263\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n#include <time.h>\n\n// 定义常量\n#define BATCH_SIZE 16\n#define IN_CHANNELS 3\n#define OUT_CHANNELS 3\n#define IMG_SIZE 64\n#define HR_IMG_SIZE 128//256//\n#define NUM_RESIDUAL_BLOCKS 4\n#define KERNEL_SIZE 3\n#define DILATION 2\n#define LEARNING_RATE 1e-4\n#define EPOCHS 100\n\n// 定义矩阵结构\ntypedef struct {\n    int rows, cols;\n    float *data;\n} Matrix;\n\n// 初始化矩阵\nMatrix create_matrix(int rows, int cols) {\n    Matrix mat;\n    mat.rows = rows;\n    mat.cols = cols;\n    mat.data = (float *)malloc(rows * cols * sizeof(float));\n    return mat;\n}\n\n// 释放矩阵内存\nvoid free_matrix(Matrix mat) {\n    free(mat.data);\n}\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat log(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\nstatic unsigned long _rand_seed = 1;\n\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n\n\n\n\n\n// 卷积操作\nMatrix conv2d(Matrix input, Matrix kernel, int stride, int padding, int dilation) {\n    int out_rows = (input.rows + 2 * padding - dilation * (kernel.rows - 1) - 1) / stride + 1;\n    int out_cols = (input.cols + 2 * padding - dilation * (kernel.cols - 1) - 1) / stride + 1;\n    Matrix output = create_matrix(out_rows, out_cols);\n\n    for (int i = 0; i < out_rows; i++) {\n        for (int j = 0; j < out_cols; j++) {\n            float sum = 0.0;\n            for (int ki = 0; ki < kernel.rows; ki++) {\n                for (int kj = 0; kj < kernel.cols; kj++) {\n                    int ii = i * stride + ki * dilation - padding;\n                    int jj = j * stride + kj * dilation - padding;\n                    if (ii >= 0 && ii < input.rows && jj >= 0 && jj < input.cols) {\n                        sum += input.data[ii * input.cols + jj] * kernel.data[ki * kernel.cols + kj];\n                    }\n                }\n            }\n            output.data[i * out_cols + j] = sum;\n        }\n    }\n    return output;\n}\n\n// 残差块\nMatrix residual_block(Matrix input) {\n    Matrix kernel = create_matrix(KERNEL_SIZE, KERNEL_SIZE);\n    for (int i = 0; i < KERNEL_SIZE * KERNEL_SIZE; i++) {\n        kernel.data[i] = (float)rand() / RAND_MAX;  // 随机初始化卷积核\n    }\n\n    Matrix conv1 = conv2d(input, kernel, 1, DILATION, DILATION);\n    Matrix conv2 = conv2d(conv1, kernel, 1, DILATION, DILATION);\n\n    for (int i = 0; i < input.rows * input.cols; i++) {\n        conv2.data[i] += input.data[i];  // 残差连接\n    }\n\n    free_matrix(kernel);\n    free_matrix(conv1);\n    return conv2;\n}\n\n// 生成器\nMatrix generator(Matrix input) {\n    Matrix output = input;\n    for (int i = 0; i < NUM_RESIDUAL_BLOCKS; i++) {\n        output = residual_block(output);\n    }\n    return output;\n}\n\n// 判别器\nMatrix discriminator(Matrix input) {\n    Matrix kernel = create_matrix(KERNEL_SIZE, KERNEL_SIZE);\n    for (int i = 0; i < KERNEL_SIZE * KERNEL_SIZE; i++) {\n        kernel.data[i] = (float)rand() / RAND_MAX;  // 随机初始化卷积核\n    }\n\n    Matrix conv1 = conv2d(input, kernel, 2, 1, 1);\n    Matrix conv2 = conv2d(conv1, kernel, 2, 1, 1);\n    Matrix conv3 = conv2d(conv2, kernel, 2, 1, 1);\n\n    free_matrix(kernel);\n    free_matrix(conv1);\n    free_matrix(conv2);\n    return conv3;\n}\n\n// 感知损失（简化版）\n// 感知损失（简化版）\ndouble perceptual_loss(Matrix hr_real, Matrix hr_fake) {\n    int min_rows = hr_real.rows < hr_fake.rows ? hr_real.rows : hr_fake.rows;\n    int min_cols = hr_real.cols < hr_fake.cols ? hr_real.cols : hr_fake.cols;\n\n    double loss = 0.0;\n    for (int i = 0; i < min_rows; i++) {\n        for (int j = 0; j < min_cols; j++) {\n            float diff = hr_real.data[i * hr_real.cols + j] - hr_fake.data[i * hr_fake.cols + j];\n            loss += pow(diff, 2);\n        }\n    }\n    return loss / (min_rows * min_cols);\n}\n\n// 对抗损失\nfloat adversarial_loss(Matrix pred, int is_real) {\n    float loss = 0.0;\n    for (int i = 0; i < pred.rows * pred.cols; i++) {\n        if (is_real) {\n            loss += -log(pred.data[i]);\n        } else {\n            loss += -log(1 - pred.data[i]);\n        }\n    }\n    return loss / (pred.rows * pred.cols);\n}\n\n// 训练函数\nvoid train(Matrix *lr_imgs, Matrix *hr_imgs, int num_images) {\n    for (int epoch = 0; epoch < EPOCHS; epoch++) {\n        for (int i = 0; i < num_images; i++) {\n            // 生成器前向传播\n            Matrix fake_hr = generator(lr_imgs[i]);\n            Matrix pred_fake = discriminator(fake_hr);\n\n            // 计算生成器损失\n            float loss_G_adv = adversarial_loss(pred_fake, 1);\n            float loss_G_perc = perceptual_loss(hr_imgs[i], fake_hr);\n            float loss_G = loss_G_adv + 0.006 * loss_G_perc;\n\n            // 判别器前向传播\n            Matrix pred_real = discriminator(hr_imgs[i]);\n            float loss_D_real = adversarial_loss(pred_real, 1);\n            float loss_D_fake = adversarial_loss(pred_fake, 0);\n            float loss_D = (loss_D_real + loss_D_fake) * 0.5;\n\n            // 打印训练日志\n           \n\n            // 释放内存\n            free_matrix(fake_hr);\n            free_matrix(pred_fake);\n            free_matrix(pred_real);\n        }\n    }\n}\n\n// 主函数\nint main() {\n   \n\n    // 模拟数据集\n    int num_images = 100;\n    Matrix lr_imgs[100];\n    Matrix hr_imgs[100];\n\n    for (int i = 0; i < 100 ; i++) {\n        lr_imgs[i] = create_matrix(IMG_SIZE, IMG_SIZE);\n        hr_imgs[i] = create_matrix(HR_IMG_SIZE, HR_IMG_SIZE);\n        for (int j = 0; j < IMG_SIZE * IMG_SIZE; j++) {\n            lr_imgs[i].data[j] = (float)rand() / RAND_MAX;  // 随机初始化低分辨率图像\n        }\n        for (int j = 0; j < HR_IMG_SIZE * HR_IMG_SIZE; j++) {\n            hr_imgs[i].data[j] = (float)rand() / RAND_MAX;  // 随机初始化高分辨率图像\n        }\n    }\n\n    // 训练模型\n    train(lr_imgs, hr_imgs, num_images);\n\n    // 释放数据集内存\n    for (int i = 0; i < num_images; i++) {\n        free_matrix(lr_imgs[i]);\n        free_matrix(hr_imgs[i]);\n    }\n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 332,\n        \"min_slack\": 1.1102230246251565e-15,\n        \"max_frequency\": 100.00000000000001,\n        \"estimated_resources_area\": 20773,\n        \"dsp_count\": 57,\n        \"flip_flops\": 6821\n    },\n    \"silicon_compiler\": {\n        \"area\": 1357340.0,\n        \"max_frequency\": 19.079636494765502,\n        \"power\": 726.4000000000001\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n#include <string.h>\n\n// 输入输出参数\n#define IN_SIZE 128//512\n#define IN_CHANNELS 3\n#define OUT_CHANNELS 1\n#define NUM_ENCODER_STAGES 5  // ResNet-50的5个阶段特征\n\n// 编码器参数（简化ResNet-50配置）\nstatic const int encoder_channels[NUM_ENCODER_STAGES] = {64, 256, 512, 1024, 2048};\nstatic const int encoder_sizes[NUM_ENCODER_STAGES] = {256, 128, 64, 32, 16}; // 各阶段特征图尺寸\n\n// 解码器参数\n#define NUM_UPSAMPLES 4\nstatic const int decoder_channels[NUM_UPSAMPLES+1] = {512, 256, 128, 64, 32};\n\n// 特征存储结构体\ntypedef struct {\n    float* data;\n    int size;\n    int channels;\n} FeatureMap;\n\n// 全局特征存储\nFeatureMap encoder_features[NUM_ENCODER_STAGES];\nFeatureMap decoder_features[NUM_UPSAMPLES+1];\n\n\n\n\n\nfloat floorf(float x) {\n    // 将浮点数转换为无符号整型以进行位操作\n    unsigned int bits = *(unsigned int*)&x;\n    int sign = bits >> 31;                     // 符号位 (0:正数, 1:负数)\n    int exponent = (bits >> 23) & 0xFF;        // 指数部分\n    int mantissa = bits & 0x007FFFFF;          // 尾数部分 (23位)\n\n    // 处理特殊情况：NaN、无穷大、零\n    if (exponent == 0xFF) return x;            // NaN或Inf\n    if (exponent == 0 && mantissa == 0) return x; // ±0.0\n\n    // 计算实际指数（减去偏移量127）\n    int exp_val = exponent - 127;\n\n    // 情况1：x的绝对值 >= 1.0\n    if (exp_val >= 23) {                       // 没有小数位（尾数全为整数）\n        return x;\n    } else if (exp_val >= 0) {                 // 有整数和小数部分\n        // 计算小数部分的位数（从右数第(23 - exp_val)位开始为小数）\n        int fractional_bits = 23 - exp_val;\n        unsigned int truncate_mask = 0xFFFFFFFF << fractional_bits;\n        unsigned int truncated = mantissa & truncate_mask;\n\n        // 构造整数部分的位表示\n        unsigned int result_bits = (sign << 31) | ((exponent) << 23) | truncated;\n        float result = *(float*)&result_bits;\n\n        // 处理负数且有小数的情况（例如-2.3 -> -3.0）\n        if (sign && (mantissa != truncated)) {\n            result -= 1.0f;\n        }\n        return result;\n\n    // 情况2：x的绝对值 < 1.0 (exp_val < 0)\n    } else {                                   \n        return (sign) ? -1.0f : 0.0f;          // 正数取0，负数取-1\n    }\n}\n\n\nfloat fmaxf(float a, float b) {\n    // 处理NaN情况（根据IEEE 754标准）\n    unsigned int a_bits = *(unsigned int*)&a;\n    unsigned int b_bits = *(unsigned int*)&b;\n    \n    // 判断a是否为NaN\n    int a_is_nan = ((a_bits >> 23) & 0xFF) == 0xFF && (a_bits & 0x007FFFFF) != 0;\n    // 判断b是否为NaN\n    int b_is_nan = ((b_bits >> 23) & 0xFF) == 0xFF && (b_bits & 0x007FFFFF) != 0;\n    \n    if (a_is_nan && b_is_nan) return a;  // 返回任意NaN\n    if (a_is_nan) return b;\n    if (b_is_nan) return a;\n    \n    return (a >= b) ? a : b;\n}\n\nfloat logf(float x) {\n    // 处理非法输入\n    if (x <= 0.0f) return 0.0f / 0.0f; // 返回NaN\n    \n    // 分解浮点数为符号、指数和尾数（仅处理正数）\n    unsigned int bits = *(unsigned int*)&x;\n    int exponent = ((bits >> 23) & 0xFF) - 127;  // 提取指数\n    \n    // 构造m ∈ [1.0, 2.0)\n    unsigned int m_bits = (bits & 0x007FFFFF) | 0x3F800000;\n    float m = *(float*)&m_bits;\n    \n    // 计算log(m)的三次多项式近似（误差<0.01）\n    float t = m - 1.0f;\n    float log_m = t * (0.9999999f \n                     - t * (0.4998741f \n                     - t * (0.3317990f \n                     - t * 0.2407338f)));\n    \n    // 组合结果：log(x) = log(m) + exponent*ln(2)\n    const float ln2 = 0.69314718056f;  // ln(2)的近似值\n    return log_m + exponent * ln2;\n}\n\n\n// double expf(double x) {\n//     double result = 1.0;\n//     double term = 1.0;\n//     for (int i = 1; i <= 10; ++i) {\n//         term *= x / i;\n//         result += term;\n//     }\n//     return result;\n// }\n\n\n\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n/******************** 核心函数实现 ​********************/\n\n// 初始化特征图内存\nFeatureMap create_feature(int size, int channels) {\n    FeatureMap fm;\n    fm.size = size;\n    fm.channels = channels;\n    fm.data = (float*)malloc(size * size * channels * sizeof(float));\n    memset(fm.data, 0, size * size * channels * sizeof(float));\n    return fm;\n}\n\n// 简化版ResNet-50编码器（仅保留下采样逻辑）\nvoid simplified_resnet50(FeatureMap input) {\n    for (int stage = 0; stage < NUM_ENCODER_STAGES; stage++) {\n        int new_size = encoder_sizes[stage];\n        int ch = encoder_channels[stage];\n        \n        // 创建特征存储\n        encoder_features[stage] = create_feature(new_size, ch);\n        \n        // 模拟卷积和下采样操作（实际应包含残差块）\n        for (int i = 0; i < new_size; i++) {\n            for (int j = 0; j < new_size; j++) {\n                for (int c = 0; c < ch; c++) {\n                    // 模拟卷积核计算（实际应使用预训练权重）\n                    float sum = 0.0f;\n                    for (int di = 0; di < 2; di++) {\n                        for (int dj = 0; dj < 2; dj++) {\n                            int input_i = i * 2 + di;\n                            int input_j = j * 2 + dj;\n                            if (input_i < input.size && input_j < input.size) {\n                                sum += input.data[(input_i * input.size + input_j) * input.channels + c % input.channels];\n                            }\n                        }\n                    }\n                    encoder_features[stage].data[(i * new_size + j) * ch + c] = sum * 0.25f; // 平均池化\n                }\n            }\n        }\n        \n        input = encoder_features[stage]; // 更新输入到下一阶段\n    }\n}\n\n// 双线性上采样\nFeatureMap upsample_bilinear(FeatureMap input) {\n    int new_size = input.size * 2;\n    FeatureMap output = create_feature(new_size, input.channels);\n    \n    for (int i = 0; i < new_size; i++) {\n        float fi = (float)i / new_size * input.size;\n        int i0 = (int)floor(fi);\n        int i1 = i0 + 1 < input.size ? i0 + 1 : i0;\n        float di = fi - i0;\n        \n        for (int j = 0; j < new_size; j++) {\n            float fj = (float)j / new_size * input.size;\n            int j0 = (int)floor(fj);\n            int j1 = j0 + 1 < input.size ? j0 + 1 : j0;\n            float dj = fj - j0;\n            \n            for (int c = 0; c < input.channels; c++) {\n                // 双线性插值\n                float v00 = input.data[(i0 * input.size + j0) * input.channels + c];\n                float v01 = input.data[(i0 * input.size + j1) * input.channels + c];\n                float v10 = input.data[(i1 * input.size + j0) * input.channels + c];\n                float v11 = input.data[(i1 * input.size + j1) * input.channels + c];\n                \n                output.data[(i * new_size + j) * output.channels + c] = \n                    v00 * (1 - di) * (1 - dj) +\n                    v01 * (1 - di) * dj +\n                    v10 * di * (1 - dj) +\n                    v11 * di * dj;\n            }\n        }\n    }\n    return output;\n}\n\n// 带跳跃连接的解码器\nFeatureMap decoder_with_skips() {\n    // 初始化解码器输入（编码器最后层特征）\n    FeatureMap current = encoder_features[NUM_ENCODER_STAGES-1];\n    \n    for (int i = 0; i < NUM_UPSAMPLES; i++) {\n        // 上采样\n        current = upsample_bilinear(current);\n        \n        // 获取对应的编码器特征\n        int encoder_stage = NUM_ENCODER_STAGES - 2 - i;\n        FeatureMap skip = encoder_features[encoder_stage];\n        \n        // 调整编码器特征尺寸（如果需要）\n        if (skip.size != current.size) {\n            skip = upsample_bilinear(skip);\n        }\n        \n        // 特征拼接\n        FeatureMap concated = create_feature(current.size, current.channels + skip.channels);\n        memcpy(concated.data, current.data, current.size * current.size * current.channels * sizeof(float));\n        memcpy(concated.data + current.size * current.size * current.channels,\n               skip.data, skip.size * skip.size * skip.channels * sizeof(float));\n        \n        // 3x3卷积（简化实现）\n        FeatureMap conv_out = create_feature(concated.size, decoder_channels[i+1]);\n        for (int h = 0; h < concated.size; h++) {\n            for (int w = 0; w < concated.size; w++) {\n                for (int oc = 0; oc < decoder_channels[i+1]; oc++) {\n                    float sum = 0.0f;\n                    for (int di = -1; di <= 1; di++) {\n                        for (int dj = -1; dj <= 1; dj++) {\n                            int nh = h + di;\n                            int nw = w + dj;\n                            if (nh >= 0 && nh < concated.size && nw >= 0 && nw < concated.size) {\n                                for (int ic = 0; ic < concated.channels; ic++) {\n                                    sum += concated.data[(nh * concated.size + nw) * concated.channels + ic] * \n                                          0.01f; // 模拟卷积核权重\n                                }\n                            }\n                        }\n                    }\n                    conv_out.data[(h * conv_out.size + w) * conv_out.channels + oc] = fmaxf(sum, 0.0f); // ReLU\n                }\n            }\n        }\n        \n        current = conv_out;\n    }\n    \n    // 最终输出层\n    FeatureMap output = create_feature(IN_SIZE, OUT_CHANNELS);\n    for (int i = 0; i < IN_SIZE; i++) {\n        for (int j = 0; j < IN_SIZE; j++) {\n            float sum = 0.0f;\n            for (int c = 0; c < current.channels; c++) {\n                sum += current.data[(i * current.size + j) * current.channels + c] * 0.1f; // 模拟权重\n            }\n            output.data[i * IN_SIZE + j] = 1.0f / (1.0f + expf(-sum)); // Sigmoid\n        }\n    }\n    return output;\n}\n\n/******************** 主程序 ​********************/\nint main() {\n    // 创建输入特征\n    FeatureMap input = create_feature(IN_SIZE, IN_CHANNELS);\n    for (int i = 0; i < IN_SIZE * IN_SIZE * IN_CHANNELS; i++) {\n        input.data[i] = 0.5f; // 示例输入\n    }\n\n    // 编码器前向传播\n    simplified_resnet50(input);\n    \n    // 解码器前向传播\n    FeatureMap output = decoder_with_skips();\n\n    // 验证输出尺寸\n   \n    // 释放内存\n    for (int i = 0; i < NUM_ENCODER_STAGES; i++) {\n        free(encoder_features[i].data);\n    }\n    free(input.data);\n    free(output.data);\n    \n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 182,\n        \"min_slack\": 1.1102230246251565e-15,\n        \"max_frequency\": 100.00000000000001,\n        \"estimated_resources_area\": 29403,\n        \"dsp_count\": 48,\n        \"flip_flops\": 4879\n    },\n    \"silicon_compiler\": {\n        \"area\": 1536680.0,\n        \"max_frequency\": 17.32345665324676,\n        \"power\": 65.1152\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n//#include <math.h>\n\n// 参数定义\n#define BATCH_SIZE 2\n#define RADAR_POINTS 1024\n#define RADAR_DIM 4\n#define IMAGE_CHANNELS 3\n#define IMAGE_SIZE 224\n#define FEATURE_DIM 512\n#define NUM_HEADS 8\n#define HEAD_DIM (FEATURE_DIM / NUM_HEADS)\n\n// 全局缓冲区（静态内存）\nfloat radar_input[BATCH_SIZE][RADAR_POINTS][RADAR_DIM];   // 雷达输入\nfloat image_input[BATCH_SIZE][IMAGE_CHANNELS][IMAGE_SIZE][IMAGE_SIZE]; // 图像输入\nfloat radar_features[BATCH_SIZE][RADAR_POINTS][FEATURE_DIM]; // 雷达特征\nfloat img_features[BATCH_SIZE][9];                        // 图像特征\nfloat fused_features[BATCH_SIZE][FEATURE_DIM];            // 融合特征\nfloat heatmap[BATCH_SIZE];                                // 热图输出\nfloat bbox[BATCH_SIZE][7];                                // 边界框输出\n\n\nfloat exp(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\n\n\n\n// 雷达编码器（4层MLP）\nvoid radar_encoder() {\n    // 模拟4层MLP（简化实现）\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int p = 0; p < RADAR_POINTS; p++) {\n            for (int c = 0; c < FEATURE_DIM; c++) {\n                radar_features[b][p][c] = 0.0f;\n                for (int i = 0; i < RADAR_DIM; i++) {\n                    radar_features[b][p][c] += radar_input[b][p][i] * 0.01f; // 模拟权重\n                }\n            }\n        }\n    }\n}\n\n// 图像编码器（简化ResNet-50）\nvoid image_encoder() {\n    // 模拟ResNet-50特征提取\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int i = 0; i < 9; i++) {\n            img_features[b][i] = 0.0f;\n            for (int c = 0; c < IMAGE_CHANNELS; c++) {\n                for (int h = 0; h < IMAGE_SIZE; h++) {\n                    for (int w = 0; w < IMAGE_SIZE; w++) {\n                        img_features[b][i] += image_input[b][c][h][w] * 0.001f; // 模拟权重\n                    }\n                }\n            }\n        }\n    }\n}\n\n// 可变形交叉注意力（简化实现）\nvoid deformable_cross_attention() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int p = 0; p < RADAR_POINTS; p++) {\n            for (int c = 0; c < FEATURE_DIM; c++) {\n                float query = 0.0f, value = 0.0f;\n                for (int i = 0; i < FEATURE_DIM; i++) {\n                    query += radar_features[b][p][i] * 0.01f; // 模拟Q权重\n                    value += img_features[b][i % 9] * 0.01f;   // 模拟V权重\n                }\n                float attention = exp(query * value / sqrt(FEATURE_DIM));\n                fused_features[b][c] += attention * value;     // 上下文聚合\n            }\n        }\n    }\n}\n\n// 检测头（热图 + 回归）\nvoid detection_head() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        heatmap[b] = 0.0f;\n        for (int c = 0; c < FEATURE_DIM; c++) {\n            heatmap[b] += fused_features[b][c] * 0.01f; // 模拟热图权重\n        }\n        heatmap[b] = 1.0f / (1.0f + exp(-heatmap[b])); // Sigmoid激活\n\n        for (int i = 0; i < 7; i++) {\n            bbox[b][i] = 0.0f;\n            for (int c = 0; c < FEATURE_DIM; c++) {\n                bbox[b][i] += fused_features[b][c] * 0.01f; // 模拟回归权重\n            }\n        }\n    }\n}\n\n// 主函数\nint main() {\n    // 初始化输入数据（模拟）\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int p = 0; p < RADAR_POINTS; p++) {\n            for (int i = 0; i < RADAR_DIM; i++) {\n                radar_input[b][p][i] = 0.5f; // 示例雷达数据\n            }\n        }\n        for (int c = 0; c < IMAGE_CHANNELS; c++) {\n            for (int h = 0; h < IMAGE_SIZE; h++) {\n                for (int w = 0; w < IMAGE_SIZE; w++) {\n                    image_input[b][c][h][w] = 0.5f; // 示例图像数据\n                }\n            }\n        }\n    }\n\n    // 执行推理\n    radar_encoder();                    // 雷达编码\n    image_encoder();                    // 图像编码\n    deformable_cross_attention();       // BEV融合\n    detection_head();                   // 检测头\n\n  \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 316,\n        \"min_slack\": 5e-10,\n        \"max_frequency\": 100.000000005,\n        \"estimated_resources_area\": 22595,\n        \"dsp_count\": 8,\n        \"flip_flops\": 7218\n    },\n    \"silicon_compiler\": {\n        \"area\": 590003.0,\n        \"max_frequency\": 16.10194463185319,\n        \"power\": 278.553\n    }\n}"
    },
    {
        "instruction": "",
        "input": "#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// 模型参数\n#define BATCH_SIZE 8\n#define IN_CHANNELS 512\n#define OUT_CHANNELS 512\n#define IN_HEIGHT 32\n#define IN_WIDTH 32\n#define RATIO 16\n#define KERNEL_SIZE 7\n#define GROUPS 8\n#define NUM_RATIOS 9\n#define BASE_SIZE 32\n\n// 全局存储\nstatic float input[BATCH_SIZE][IN_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float loc_map[BATCH_SIZE][1][IN_HEIGHT][IN_WIDTH];\nstatic float anchors[BATCH_SIZE][NUM_RATIOS][2];  // 9种比例的宽高\nstatic float aligned_feat[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT][IN_WIDTH];\nstatic float fpn_output[BATCH_SIZE][OUT_CHANNELS][IN_HEIGHT * 2][IN_WIDTH * 2];\n\nfloat expf(float x) {\n    float result = 1.0f;      // 初始项 e^0 = 1\n    float term = 1.0f;         // 当前项初始值\n    const int max_iter = 15;  // 基于float精度调整迭代次数\n\n    for (int i = 1; i <= max_iter; ++i) {\n        term *= x / (float)i; // 显式转换为float避免整数除法\n        result += term;\n        \n        // 提前终止条件：当新增项小于float精度阈值\n        if (term < 1e-7f && term > -1e-7f) break;\n    }\n    return result;\n}\n\nstatic unsigned long _rand_seed = 1;\n\nint rand() {\n    _rand_seed = _rand_seed * 1103515245 + 12345;\n    return (unsigned int)(_rand_seed / 65536) % 32768;\n}\n\n// 初始化输入数据\nvoid init_input() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    input[b][c][h][w] = (float)rand() / RAND_MAX;  // 随机初始化\n                }\n            }\n        }\n    }\n}\n\n// 动态Anchor生成\nvoid guided_anchoring() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < IN_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    // 位置概率图\n                    loc_map[b][0][h][w] = 1.0 / (1.0 + expf(-input[b][c][h][w]));  // Sigmoid\n                }\n            }\n        }\n    }\n\n    // 生成9种比例的宽高\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int i = 0; i < NUM_RATIOS; i++) {\n            anchors[b][i][0] = BASE_SIZE * expf(0.01 * input[b][i * 2][0][0]);      // 宽\n            anchors[b][i][1] = BASE_SIZE * expf(0.01 * input[b][i * 2 + 1][0][0]);   // 高\n        }\n    }\n}\n\n// 改进型RoI Align\nvoid enhanced_roi_align() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT; h++) {\n                for (int w = 0; w < IN_WIDTH; w++) {\n                    // 双线性插值\n                    float sum = 0.0;\n                    for (int kh = 0; kh < KERNEL_SIZE; kh++) {\n                        for (int kw = 0; kw < KERNEL_SIZE; kw++) {\n                            int ih = h + kh - KERNEL_SIZE / 2;\n                            int iw = w + kw - KERNEL_SIZE / 2;\n                            if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                sum += input[b][c][ih][iw] * 0.01;  // 假设权重为0.01\n                            }\n                        }\n                    }\n                    aligned_feat[b][c][h][w] = sum / (KERNEL_SIZE * KERNEL_SIZE);\n                }\n            }\n        }\n    }\n}\n\n// 多级FPN特征裁剪\nvoid multi_fpn_extract() {\n    for (int b = 0; b < BATCH_SIZE; b++) {\n        for (int c = 0; c < OUT_CHANNELS; c++) {\n            for (int h = 0; h < IN_HEIGHT * 2; h++) {\n                for (int w = 0; w < IN_WIDTH * 2; w++) {\n                    // 特征融合\n                    float sum = 0.0;\n                    for (int ic = 0; ic < IN_CHANNELS / GROUPS; ic++) {\n                        for (int kh = 0; kh < 3; kh++) {\n                            for (int kw = 0; kw < 3; kw++) {\n                                int ih = h / 2 + kh - 1;\n                                int iw = w / 2 + kw - 1;\n                                if (ih >= 0 && ih < IN_HEIGHT && iw >= 0 && iw < IN_WIDTH) {\n                                    sum += aligned_feat[b][ic][ih][iw] * 0.01;  // 假设权重为0.01\n                                }\n                            }\n                        }\n                    }\n                    fpn_output[b][c][h][w] = sum;\n                }\n            }\n        }\n    }\n}\n\n// 主函数\nint main() {\n    // 初始化输入\n    init_input();\n\n    // 动态Anchor生成\n    guided_anchoring();\n\n    // 改进型RoI Align\n    enhanced_roi_align();\n\n    // 多级FPN特征裁剪\n    multi_fpn_extract();\n\n    // 打印输出\n    \n\n    return 0;\n}",
        "output": "{\n    \"bambu\": {\n        \"control_steps\": 14,\n        \"min_slack\": 1.511000000000004,\n        \"max_frequency\": 117.79950524207804,\n        \"estimated_resources_area\": 1228,\n        \"dsp_count\": 3,\n        \"flip_flops\": 377\n    },\n    \"silicon_compiler\": {\n        \"area\": 29689.7,\n        \"max_frequency\": 27.89563684344132,\n        \"power\": 2.17652\n    }\n}"
    }
]